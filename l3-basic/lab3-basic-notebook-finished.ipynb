{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPF5xsZxy9UV"
      },
      "source": [
        "# L3: Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzKRoslVy9UW"
      },
      "source": [
        "In this lab, you will implement the encoder‚Äìdecoder architecture of [Sutskever et al., 2014](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf), including the attention-based extension presented of [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473), and evaluate this architecture on a machine translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvJWP0ffy9UY"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUckMzs1y9UY"
      },
      "source": [
        "Training the models in this notebook requires significant compute power, and we strongly recommend using a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYpvFbvXy9UY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT140Tj9y9UZ"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQaAR47By9UZ"
      },
      "source": [
        "We will build a system that translates from German (our **source language**) to English (our **target language**). The dataset is a collection of parallel English‚ÄìGerman sentences taken from translations of subtitles for TED talks. It was derived from the [TED2013](https://opus.nlpl.eu/TED2013-v1.1.php) dataset, which is available in the [OPUS](http://opus.nlpl.eu/) collection. The code cell below prints the first lines in the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJkFna3Dy9UZ",
        "outputId": "0aac9897-a431-454b-b27c-21e55c11c1ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: david gallo : das ist bill lange . ich bin dave gallo . / david gallo : this is bill lange . i 'm dave gallo .\n",
            "1: wir werden ihnen einige geschichten √ºber das meer in videoform erz√§hlen . / and we 're going to tell you some stories from the sea here in video .\n",
            "2: ich denke , das problem ist , dass wir das meer f√ºr zu selbstverst√§ndlich halten . / and the problem , i think , is that we take the ocean for granted .\n",
            "3: wenn man dar√ºber nachdenkt , machen die ozeane 75 % des planeten aus . / when you think about it , the oceans are 75 percent of the planet .\n",
            "4: der gro√üteil der erde ist meerwasser . / most of the planet is ocean water .\n"
          ]
        }
      ],
      "source": [
        "with open('train-de.txt') as src, open('train-en.txt') as tgt:\n",
        "    for i, src_sentence, tgt_sentence in zip(range(5), src, tgt):\n",
        "        print(f'{i}: {src_sentence.rstrip()} / {tgt_sentence.rstrip()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ9ScCQ9y9Ua"
      },
      "source": [
        "As you can see, some ‚Äúsentences‚Äù are actually *sequences* of sentences, but we will use the term *sentence* nevertheless. All sentences are whitespace-tokenised and lowercased. To make your life a bit easier, we have removed sentences longer than 25 words.\n",
        "\n",
        "The next cell contains code that yields the sentences contained in a file as lists of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVMl8tB8y9Ua"
      },
      "outputs": [],
      "source": [
        "def sentences(filename):\n",
        "    with open(filename) as source:\n",
        "        for line in source:\n",
        "            yield line.rstrip().split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRBHtE8Py9Ub"
      },
      "source": [
        "## Problem 1: Build the vocabularies (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVncb3Vky9Ub"
      },
      "source": [
        "Your first task is to build the vocabularies for the data, one vocabulary for each language. Each vocabulary should contain the 10,000 most frequent words in the training data for the respective language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-LKMlQey9Uc"
      },
      "outputs": [],
      "source": [
        "def make_vocab(sentences, max_size):\n",
        "    vocab_dict = {'<pad>': 0, '<bos>': 1, '<eos>':2, '<unk>': 3}\n",
        "    frequency_dict = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word in vocab_dict:\n",
        "                frequency_dict[word] += 1\n",
        "            else:\n",
        "                frequency_dict[word] = 1\n",
        "    # sort frequency dict by value\n",
        "    frequency_dict = dict(sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "    for word in frequency_dict:\n",
        "        if len(vocab_dict) < max_size:\n",
        "            vocab_dict[word] = len(vocab_dict)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return vocab_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aveq2iVAy9Uc"
      },
      "source": [
        "Your implementation must comply with the following specification:\n",
        "\n",
        "**make_vocab** (*sentences*, *max_size*)\n",
        "\n",
        "> Returns a dictionary that maps the most frequent words in the *sentences* to a contiguous range of integers starting at&nbsp;0. The first four mappings in this dictionary are reserved for the pseudowords `<pad>` (padding, id&nbsp;0), `<bos>` (beginning of sequence, id&nbsp;1), `<eos>` (end of sequence, id&nbsp;2), and `<unk>` (unknown word, id&nbsp;3). The parameter *max_size* caps the size of the dictionary, including the pseudowords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPDqt_Fby9Uc"
      },
      "source": [
        "With this function, we can construct the vocabularies as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1mZAYfmy9Uc",
        "outputId": "de0a82d8-1f7c-4cb3-de12-729a2770274c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "src_vocab = make_vocab(sentences('train-de.txt'), 10000)\n",
        "print(len(src_vocab))\n",
        "tgt_vocab = make_vocab(sentences('train-en.txt'), 10000)\n",
        "print(len(tgt_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUVUqRwQy9Ud"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "To test you code, check that each vocabulary contains 10,000 words, including the pseudowords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtodpS1Dy9Ud",
        "outputId": "0965e511-662a-411e-83a6-0ef26cd0fff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 2 3\n",
            "0 1 2 3\n"
          ]
        }
      ],
      "source": [
        "print(src_vocab['<pad>'], src_vocab['<bos>'], src_vocab['<eos>'], src_vocab['<unk>'])\n",
        "print(tgt_vocab['<pad>'], tgt_vocab['<bos>'], tgt_vocab['<eos>'], tgt_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mUO8qr-y9Ud"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k404ZsHvy9Ud"
      },
      "source": [
        "The next cell defines a class for the parallel dataset. We sub-class the abstract [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class, which represents map-style datasets in PyTorch. This will let us use standard infrastructure related to the loading and automatic batching of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSm1ooaoy9Ue"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, src_vocab, src_filename, tgt_vocab, tgt_filename):\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "        # We hard-wire the codes for <bos> (1), <eos> (2), and <unk> (3).\n",
        "        self.src = [[self.src_vocab.get(w, 3) for w in s] for s in sentences(src_filename)]\n",
        "        self.tgt = [[self.tgt_vocab.get(w, 3) for w in s] + [2] for s in sentences(tgt_filename)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src[idx], self.tgt[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfsXtety9Ue"
      },
      "source": [
        "We load the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqa_42woy9Ue"
      },
      "outputs": [],
      "source": [
        "train_dataset = TranslationDataset(src_vocab, 'train-de.txt', tgt_vocab, 'train-en.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEnMiGG_y9Ue"
      },
      "source": [
        "The following function will be helpful for debugging. It extracts a single source‚Äìtarget pair of sentences from the specified *dataset* and converts it into batches of size&nbsp;1, which can be fed into the encoder‚Äìdecoder model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jxs4sh6y9Ue"
      },
      "outputs": [],
      "source": [
        "def example(dataset, i):\n",
        "    src, tgt = dataset[i]\n",
        "    return torch.LongTensor(src).unsqueeze(0), torch.LongTensor(tgt).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seqdcOTMy9Ue",
        "outputId": "a88c51ab-ec2c-4206-8b1d-55f0c0ddd64f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  5, 11]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  5, 11,  2]]))"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "example(train_dataset, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om83EMPty9Uf"
      },
      "source": [
        "## Problem 2: The encoder‚Äìdecoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1yC2QhIy9Uf"
      },
      "source": [
        "In this section, you will implement the encoder‚Äìdecoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f44IeCTny9Uf"
      },
      "source": [
        "### Problem 2.1: Implement the encoder (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmHdTM6y9Uf"
      },
      "source": [
        "The encoder is relatively straightforward. We look up word embeddings and unroll a bidirectional GRU over the embedding vectors to compute a representation at each token position. We then take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a summary of the source sentence, which we will later feed into the decoder.\n",
        "\n",
        "To solve this problem, complete the skeleton code in the next code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umYZDrdSy9Uf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_words, embedding_dim=256, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
        "        # Bidirectional GRU layer\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        # Final linear layer\n",
        "        self.linear = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, src: torch.LongTensor):\n",
        "        '''\n",
        "        Input tensor has shape (batch_size, src_len)\n",
        "        '''\n",
        "        # Forward pass\n",
        "        embedded = self.embedding(src)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        # Hidden shape: (2, src_len, hidden_dim)\n",
        "        # print(\"Hidden tensor shape: \", hidden.shape)\n",
        "\n",
        "        # Output shape: (batch_size, src_len, 2 * hidden_dim)\n",
        "        # print(\"Output tensor shape: \", output.shape)\n",
        "\n",
        "        # Output should have shape (batch_size, src_len, hidden_dim)\n",
        "        output = self.linear(output)\n",
        "\n",
        "        # Hidden should have shape (batch_size, hidden_dim)\n",
        "        hidden = torch.tanh(self.linear(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64eJcynAy9Ug"
      },
      "source": [
        "Your code must comply with the following specification:\n",
        "\n",
        "**__init__** (*num_words*, *embedding_dim* = 256, *hidden_dim* = 512)\n",
        "\n",
        "> Initialises the encoder. The encoder consists of an embedding layer that maps each of *num_words* words to an embedding vector of size *embedding_dim*, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 √ó *hidden_dim*, and a final linear layer that projects these representationcons to new representations of size *hidden_dim*.\n",
        "\n",
        "**forward** (*self*, *src*)\n",
        "\n",
        "> Takes a tensor *src* with source-language word ids and sends it through the encoder. The input tensor has shape (*batch_size*, *src_len*), where *src_len* is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (*output*, *hidden*), where *output* has shape (*batch_size*, *src_len*, *hidden_dim*), and *hidden* has shape (*batch_size*, *hidden_dim*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57J4Cpmky9Ug"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "To test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSk5TxyAy9Ug"
      },
      "source": [
        "### Problem 2.2: Implement the attention mechanism (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZT53Atly9Ug",
        "outputId": "fadde30a-6c2d-41ae-d876-176c6a620d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 14])\n",
            "torch.Size([10, 14, 512]) torch.Size([10, 512])\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(len(src_vocab))\n",
        "example_input = example(train_dataset, 3)[0]\n",
        "example_input = torch.zeros(10, 14, dtype=torch.long)\n",
        "print(example_input.shape)\n",
        "output, hidden = encoder.forward(example_input)\n",
        "print(output.shape, hidden.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBsKdJk7y9Ug"
      },
      "source": [
        "Your next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called *context* vector. For later usage, we also return the attention weights.\n",
        "\n",
        "Attention can be implemented in various ways. One very simple implementation is *uniform attention*, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aCxWYhmy9Ug"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class UniformAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_output, src_mask):\n",
        "        batch_size, src_len, _ = encoder_output.shape\n",
        "\n",
        "        # Set all attention scores to the same constant value (0). After\n",
        "        # the softmax, we will have uniform weights.\n",
        "        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n",
        "\n",
        "        print(\"Scores shape: \", scores.shape)\n",
        "        print(\"Src mask shape: \", src_mask.shape)\n",
        "\n",
        "        # Mask out the attention scores for the padding tokens. We set\n",
        "        # them to -inf. After the softmax, we will have 0.\n",
        "        scores.data.masked_fill_(~src_mask, -float('inf'))\n",
        "\n",
        "        # Convert scores into weights\n",
        "        alpha = F.softmax(scores, dim=1)\n",
        "\n",
        "        # The context is the alpha-weighted sum of the encoder outputs.\n",
        "        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n",
        "\n",
        "        return context, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlF1cpgny9Uh"
      },
      "source": [
        "One technical detail in this code is our use of a mask *src_mask* to compute attention weights only for the ‚Äúreal‚Äù tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length.\n",
        "\n",
        "Your task now is to implement the attention mechanism from the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473). The relevant equation is in Section&nbsp;A.1.2:\n",
        "\n",
        "$$\n",
        "a(s_{i-1}, h_j) = v^{\\top} \\tanh(W s_{i-1} + U h_j)\n",
        "$$\n",
        "\n",
        "This equation specifies how to compute the attention score (a scalar) for the previous hidden state of the decoder, denoted by&nbsp;$s_{i-1}$, and the $j$th position-specific representation in the output of the encoder, denoted by&nbsp;$h_j$. The equation refers to three parameters: a vector $v$ and $W$ and $U$. In PyTorch, these parameters can be represented in terms of (bias-free) linear layers that are trained along with the other parameters of the model.\n",
        "\n",
        "Here is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (*scores*); the rest of the code is the same as for the uniform attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO0qzz_cy9Uh"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.U = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, decoder_hidden: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor):\n",
        "        #batch_size, src_len, _ = encoder_output.shape\n",
        "\n",
        "        scores = self.v(torch.tanh(self.W(decoder_hidden).unsqueeze(1) + self.U(encoder_output)))\n",
        "        # Remove the extra dimension from scores\n",
        "        scores = scores.squeeze(-1)\n",
        "\n",
        "        #print(\"Scores shape: \", scores.shape)\n",
        "        #print(\"Src mask shape: \", src_mask.shape)\n",
        "\n",
        "        # The rest of the code is as in UniformAttention\n",
        "\n",
        "        # Mask out the attention scores for the padding tokens. We set\n",
        "        # them to -inf. After the softmax, we will have 0.\n",
        "        scores.data.masked_fill_(~src_mask, -float('inf'))\n",
        "\n",
        "        # Convert scores into weights\n",
        "        alpha = F.softmax(scores, dim=1)\n",
        "\n",
        "        # The context vector is the alpha-weighted sum of the encoder outputs.\n",
        "        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n",
        "\n",
        "        return context, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM4Blikqy9Uh"
      },
      "source": [
        "Your code must comply with the following specification:\n",
        "\n",
        "**forward** (*decoder_hidden*, *encoder_output*, *src_mask*)\n",
        "\n",
        "> Takes the previous hidden state of the decoder (*decoder_hidden*) and the encoder output (*encoder_output*) and returns a pair (*context*, *alpha*) where *context* is the context as computed as in [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473), and *alpha* are the corresponding attention weights. The hidden state has shape (*batch_size*, *hidden_dim*), the encoder output has shape (*batch_size*, *src_len*, *hidden_dim*), the context has shape (*batch_size*, *hidden_dim*), and the attention weights have shape (*batch_size*, *src_len*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWy54Yqvy9Uh"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "To test your code, extend your test from Problem&nbsp;2.1: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. You will also need to create a source mask; this can be done as follows:\n",
        "\n",
        "```\n",
        "src_mask = (src != 0)\n",
        "```\n",
        "\n",
        "Check that the context tensor and the attention weights returned by the attention class have the expected shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBnEFVSNy9Uh",
        "outputId": "ec5d39e6-5405-4f12-b13a-e12270234908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512]) torch.Size([1, 13])\n"
          ]
        }
      ],
      "source": [
        "# Test the attention mechanism\n",
        "attention = BahdanauAttention()\n",
        "src = example(train_dataset, 0)[0]\n",
        "output, hidden = encoder(src)\n",
        "src_mask = (src != 0)\n",
        "context, alpha = attention(hidden, output, src_mask)\n",
        "print(context.shape, alpha.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uu6dbZSy9Ui"
      },
      "source": [
        "### Problem 2.3: Implement the decoder (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOj4_EOpy9Ui"
      },
      "source": [
        "Now you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n",
        "\n",
        "**‚ö†Ô∏è We expect that solving this problem will take you the longest time in this lab.**\n",
        "\n",
        "Because the decoder is an autoregressive model, we need to unroll the GRU ‚Äúmanually‚Äù: At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n",
        "\n",
        "<img src=\"https://gitlab.liu.se/nlp/nlp-course/-/raw/master/labs/l3/decoder.svg\" width=\"50%\" alt=\"Decoder architecture\"/>\n",
        "\n",
        "We need to implement this manual unrolling for two very similar tasks: When *training*, both the inputs to and the target outputs of the GRU come from the training data. When *decoding*, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. We have implemented methods `forward` and `decode` for these two different modes of usage. Your task is to implement a method `step` that takes a single step with the GRU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47O985GSy9Ui"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_words, attention, embedding_dim=256, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
        "        self.attention = attention\n",
        "        self.rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(embedding_dim + 2 * hidden_dim, num_words)\n",
        "\n",
        "    def forward(self, encoder_output, hidden, src_mask, tgt):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "\n",
        "        # Lookup the embeddings for the previous words\n",
        "        embedded = self.embedding(tgt)\n",
        "\n",
        "        # Initialise the list of outputs (in each sentence)\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(tgt_len):\n",
        "            # Get the embedding for the previous word (in each sentence)\n",
        "            prev_embedded = embedded[:, i]\n",
        "\n",
        "            # Take one step with the RNN\n",
        "            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n",
        "\n",
        "            # Update the list of outputs (in each sentence)\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def decode(self, encoder_output, hidden, src_mask, max_len):\n",
        "        batch_size = encoder_output.size(0)\n",
        "\n",
        "        # Initialise the list of generated words and attention weights (in each sentence)\n",
        "        generated = [torch.ones(batch_size, dtype=torch.long, device=hidden.device)]\n",
        "        alphas = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # Get the embedding for the previous word (in each sentence)\n",
        "            prev_embedded = self.embedding(generated[-1])\n",
        "\n",
        "            # Take one step with the RNN\n",
        "            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n",
        "\n",
        "            # Update the list of generated words and attention weights (in each sentence)\n",
        "            generated.append(output.argmax(-1))\n",
        "            alphas.append(alpha)\n",
        "\n",
        "        generated = [x.unsqueeze(1) for x in generated[1:]]\n",
        "        alphas = [x.unsqueeze(1) for x in alphas]\n",
        "\n",
        "        return torch.cat(generated, dim=1), torch.cat(alphas, dim=1)\n",
        "\n",
        "    def step(self, encoder_output, hidden, src_mask, prev_embedded):\n",
        "        # Compute the context vector and attention weights\n",
        "        context, alpha = self.attention(hidden, encoder_output, src_mask)\n",
        "\n",
        "        rnn_input = torch.cat((prev_embedded, context), dim=1)\n",
        "\n",
        "        # Take one step with the RNN\n",
        "        rnn_output, hidden = self.rnn(rnn_input.unsqueeze(1), hidden.unsqueeze(0))\n",
        "\n",
        "        linear_input = torch.cat((rnn_output.squeeze(1), rnn_input), dim=1)\n",
        "\n",
        "        # Compute the output\n",
        "        linear_output = self.linear(linear_input)\n",
        "\n",
        "        return linear_output, hidden.squeeze(0), alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAX3RX93y9Ui"
      },
      "source": [
        "Your implementation should comply with the following specification:\n",
        "\n",
        "**step** (*self*, *encoder_output*, *hidden*, *src_mask*, *prev_embedded*)\n",
        "\n",
        "> Performs a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (*encoder_output*), the previous hidden state of the decoder (*hidden*), the source mask as described in Problem&nbsp;2.2 (*src_mask*), and the embedding vector of the previous word (*prev_embedded*), and computes the output as described above.\n",
        ">\n",
        "> The shape of *encoder_output* is (*batch_size*, *src_len*, *hidden_dim*); the shape of *hidden* is (*batch_size*, *hidden_dim*); the shape of *src_mask* is (*batch_size*, *src_len*); and the shape of *prev_embedded* is (*batch_size*, *embedding_dim*).\n",
        ">\n",
        "> The method returns a triple of tensors (*output*, *hidden*, *alpha*) where *output* is the position-specific output of the GRU, of shape (*batch_size*, *num_words*); *hidden* is the new hidden state, of shape (*batch_size*, *hidden_dim*); and *alpha* are the attention weights that were used to compute the *output*, of shape (*batch_size*, *src_len*).\n",
        "\n",
        "#### üí° Hints on the implementation\n",
        "\n",
        "**Batch first!** Per default, the GRU implementation in PyTorch (just as the LSTM implementation) expects its input to be a three-dimensional tensor of the form (*seq_len*, *batch_size*, *input_size*). We find it conceptually easier to change this default behaviour and let the models take their input in the form (*batch_size*, *seq_len*, *input_size*). To do so, set *batch_first=True* when instantiating the GRU.\n",
        "\n",
        "**Unsqueeze and squeeze.** When doing the unrolling manually, we get the input in the form (*batch_size*, *input_size*). To convert between this representation and the (*batch_size*, *seq_len*, *input_size*) representation, you can use [`unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) and [`squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UlbHCswy9Ui"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "To test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder‚Äìdecoder architecture on the example sentence. Check the shapes of the resulting tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL5-Pummy9Ui",
        "outputId": "39ba6edb-33d3-4be6-92e3-b0b4857cf75b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 14, 10000])\n"
          ]
        }
      ],
      "source": [
        "attention = BahdanauAttention()\n",
        "encoder = Encoder(len(src_vocab))\n",
        "decoder = Decoder(len(tgt_vocab), attention)\n",
        "src = example(train_dataset, 0)[0]\n",
        "output, hidden = encoder.forward(src)\n",
        "src_mask = (src != 0)\n",
        "tgt = example(train_dataset, 0)[1]\n",
        "output = decoder(encoder_output=output, hidden=hidden, src_mask=src_mask, tgt=tgt)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi7wnRtby9Uj"
      },
      "source": [
        "### Encoder‚Äìdecoder wrapper class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GJJL-b7y9Uj"
      },
      "source": [
        "The last part of the implementation is a class that wraps the encoder and the decoder as a single model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4j6iOzqy9Uj"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, attention):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size)\n",
        "        self.decoder = Decoder(tgt_vocab_size, attention)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        encoder_output, hidden = self.encoder(src)\n",
        "        return self.decoder.forward(encoder_output, hidden, src != 0, tgt)\n",
        "\n",
        "    def decode(self, src, max_len):\n",
        "        encoder_output, hidden = self.encoder(src)\n",
        "        return self.decoder.decode(encoder_output, hidden, src != 0, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHu6VTRhy9Uj"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "As a final test, instantiate an encoder‚Äìdecoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsFb2J7xy9Uj",
        "outputId": "2d328a6e-9b3c-48b7-9c01-30d0576a114b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512]) torch.Size([1, 512, 13])\n"
          ]
        }
      ],
      "source": [
        "encoder_decorder = EncoderDecoder(len(src_vocab), len(tgt_vocab), BahdanauAttention())\n",
        "src = example(train_dataset, 0)[0]\n",
        "output, alpha = encoder_decorder.decode(src, 512)\n",
        "print(output.shape, alpha.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrxdOxvy9Uj"
      },
      "source": [
        "## Problem 3: Train a translator (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ26E4Qcy9Uj"
      },
      "source": [
        "We now have all the pieces to build and train a complete translation system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbXUIaJy9Uj"
      },
      "source": [
        "### Translator class\n",
        "\n",
        "We first define a class `Translator` that initialises an encoder‚Äìdecoder model and uses it to translate sentences. It can also return the attention weights that were used to produce the translation of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr890x4yy9Uk"
      },
      "outputs": [],
      "source": [
        "class Translator(object):\n",
        "\n",
        "    def __init__(self, src_vocab, tgt_vocab, attention, device=torch.device('cuda')):\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.device = device\n",
        "        self.model = EncoderDecoder(len(src_vocab), len(tgt_vocab), attention).to('cuda')\n",
        "\n",
        "    def translate_with_attention(self, sentences):\n",
        "        # Encode each sentence\n",
        "        encoded = [[self.src_vocab.get(w, 3) for w in s.split()] for s in sentences]\n",
        "\n",
        "        # Determine the maximal length of an encoded sentence\n",
        "        max_len = max(len(e) for e in encoded)\n",
        "\n",
        "        # Build the input tensor, padding all sequences to the same length\n",
        "        src = torch.LongTensor([e + [0] * (max_len - len(e)) for e in encoded]).to(self.device)\n",
        "\n",
        "        # Run the decoder and convert the result into nested lists\n",
        "        with torch.no_grad():\n",
        "            decoded, alphas = tuple(d.cpu().numpy().tolist() for d in self.model.decode(src, 2 * max_len))\n",
        "\n",
        "        # Prune each decoded sentence after the first <eos>\n",
        "        i2w = {i: w for w, i in self.tgt_vocab.items()}\n",
        "        result = []\n",
        "        for d, a in zip(decoded, alphas):\n",
        "            d = [i2w[i] for i in d]\n",
        "            try:\n",
        "                eos_index = d.index('<eos>')\n",
        "                del d[eos_index:]\n",
        "                del a[eos_index:]\n",
        "            except:\n",
        "                pass\n",
        "            result.append((' '.join(d), a))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def translate(self, sentences):\n",
        "        translated, alphas = zip(*self.translate_with_attention(sentences))\n",
        "        return translated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ni0yQ_Dy9Uk"
      },
      "source": [
        "The code below shows how this class is supposed to be used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmAfgH8By9Uk",
        "outputId": "3c77f21a-61b1-4896-afb1-066034697033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('nagging volatility replacements certain lake comprised cubes hostility clouds survived',\n",
              " 'nagging volatility replacements finale dirt grief carried marriage deliberately abolish')"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "translator = Translator(src_vocab, tgt_vocab, BahdanauAttention())\n",
        "translator.translate(['ich wei√ü nicht .', 'das haus ist klein .'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ_IpVk1y9Uk"
      },
      "source": [
        "### Evaluation function\n",
        "\n",
        "Machine translation systems are typically evaluated using the BLEU metric. Here we use the implementation of this metric from the `sacrebleu` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbva7hyLy9Uk",
        "outputId": "d1fbd6c2-5017-41dc-c8b5-49bb0c8526c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ],
      "source": [
        "# If sacrebleu is not found, uncomment the next line:\n",
        "!pip install sacrebleu\n",
        "\n",
        "import sacrebleu\n",
        "\n",
        "def bleu(translator, src, ref):\n",
        "    translated = translator.translate(src)\n",
        "    return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Yqfn1Cy9Uk"
      },
      "source": [
        "We will report the BLEU score on the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwhynKu2y9Uk"
      },
      "outputs": [],
      "source": [
        "with open('valid-de.txt') as src, open('valid-en.txt') as ref:\n",
        "    valid_src = [line.rstrip() for line in src]\n",
        "    valid_ref = [line.rstrip() for line in ref]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU6HhblHy9Ul"
      },
      "source": [
        "### Batcher class\n",
        "\n",
        "To prepare the training, we next create a class that takes a batch of encoded parallel sentences (a pair of lists of integers) and transforms it into two tensors, one for the source side and one for the target side. Each tensor contains sequences padded to the length of the longest sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02m3gf6Vy9Ul"
      },
      "outputs": [],
      "source": [
        "class TranslationBatcher(object):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        srcs, tgts = zip(*batch)\n",
        "\n",
        "        # Determine the maximal length of a source/target sequence\n",
        "        max_src_len = max(len(s) for s in srcs)\n",
        "        max_tgt_len = max(len(t) for t in tgts)\n",
        "\n",
        "        # Create the source/target tensors\n",
        "        S = torch.LongTensor([s + [0] * (max_src_len - len(s)) for s in srcs])\n",
        "        T = torch.LongTensor([t + [0] * (max_tgt_len - len(t)) for t in tgts])\n",
        "\n",
        "        return S.to(self.device), T.to(self.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U9eaNEny9Ul"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "The training loop resembles the training loops that you have seen in previous labs, except that we use a few new utilities from the PyTorch ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaMYMp4Ny9Ul"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(n_epochs=2, batch_size=128, lr=5e-4):\n",
        "    # Build the vocabularies\n",
        "    vocab_src = make_vocab(sentences('train-de.txt'), 10000)\n",
        "    vocab_tgt = make_vocab(sentences('train-en.txt'), 10000)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset = TranslationDataset(vocab_src, 'train-de.txt', vocab_tgt, 'train-en.txt')\n",
        "\n",
        "    # Prepare the data loaders\n",
        "    batcher = TranslationBatcher(device)\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=batcher)\n",
        "\n",
        "    # Build the translator\n",
        "    translator = Translator(src_vocab, tgt_vocab, BahdanauAttention())\n",
        "\n",
        "    # Initialise the optimiser\n",
        "    optimizer = torch.optim.Adam(translator.model.parameters(), lr=lr)\n",
        "\n",
        "    # Make it possible to interrupt the training\n",
        "    try:\n",
        "        for epoch in range(n_epochs):\n",
        "            losses = []\n",
        "            bleu_valid = 0\n",
        "            sample = '<none>'\n",
        "            with tqdm(total=len(train_dataset)) as pbar:\n",
        "                for i, (src_batch, tgt_batch) in enumerate(train_loader):\n",
        "                    # Create a shifted version of tgt_batch containing the previous words\n",
        "                    batch_size, tgt_len = tgt_batch.shape\n",
        "                    bos = torch.ones(batch_size, 1, dtype=torch.long, device=tgt_batch.device)\n",
        "                    tgt_batch_shifted = torch.cat((bos, tgt_batch[:, :-1]), dim=1)\n",
        "\n",
        "                    translator.model.train()\n",
        "\n",
        "                    # Forward pass\n",
        "                    scores = translator.model(src_batch, tgt_batch_shifted)\n",
        "                    scores = scores.view(-1, len(tgt_vocab))\n",
        "\n",
        "                    # Backward pass\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = F.cross_entropy(scores, tgt_batch.view(-1), ignore_index=0)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Update the diagnostics\n",
        "                    losses.append(loss.item())\n",
        "                    pbar.set_postfix(loss=(sum(losses) / len(losses)), bleu_valid=bleu_valid, sample=sample)\n",
        "                    pbar.update(len(src_batch))\n",
        "\n",
        "                    if i % 50 == 0:\n",
        "                        translator.model.eval()\n",
        "                        bleu_valid = int(bleu(translator, valid_src, valid_ref))\n",
        "                        sample = translator.translate(['das haus ist klein .'])[0]\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "    return translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7qbuMjy9Ul"
      },
      "source": [
        "Now it is time to train the system. During training, two diagnostics will be printed periodically: the running average of the training loss, the BLEU score on the validation data, and the translation of a sample sentence, *das haus ist klein* (which should translate into *the house is small*).\n",
        "\n",
        "As mentioned before, training the translator takes quite a bit of compute power and time. Even with a GPU, you should expect training times per epoch of about 8‚Äì10 minutes. The default number of epochs is&nbsp;2; however, you may want to interrupt the training prematurely and use a partially trained model in case you run out of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USHL6J2Py9Ul",
        "outputId": "843205b1-0e0c-4595-9226-8ba26dc6a0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143121/143121 [03:52<00:00, 616.07it/s, bleu_valid=15, loss=3.62, sample=it 's a small thing .]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143121/143121 [03:50<00:00, 619.94it/s, bleu_valid=19, loss=2.6, sample=it 's a house .]\n"
          ]
        }
      ],
      "source": [
        "translator = train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-pzE9sty9Ul"
      },
      "source": [
        "**‚ö†Ô∏è Your submitted notebook must contain output demonstrating at least 16 BLEU points on the validation data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1Atjpb-y9Ul"
      },
      "source": [
        "## Problem 4: Visualising attention (6 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUJxflpAy9Um"
      },
      "source": [
        "Figure&nbsp;3 in the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function `plot_attention` that visualises the attention weights. The *x* axis corresponds to the words in the source sentence (German) and the *y* axis to the generated target sentence (English). The heatmap colours represent the strengths of the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnAFVKy6y9Um",
        "outputId": "37670432-4503-4fee-d793-acb6a9f6a56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-192-c887944f02de>:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn')\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "def plot_attention(translator, sentence):\n",
        "    translation, weights = translator.translate_with_attention([sentence])[0]\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(weights, cmap='Blues_r')\n",
        "\n",
        "    ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
        "    ax.set_yticklabels(translation.split(), minor=False)\n",
        "\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJWCeZN9y9Um"
      },
      "source": [
        "Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_z65wray9Um",
        "outputId": "38aa1e34-6758-4515-f80e-6f2ae995f845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-192-c887944f02de>:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
            "<ipython-input-192-c887944f02de>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(translation.split(), minor=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"464.22975pt\" height=\"348.002813pt\" viewBox=\"0 0 464.22975 348.002813\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-09T08:23:36.862736</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 348.002813 \nL 464.22975 348.002813 \nL 464.22975 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 41.44375 340.802813 \nL 398.56375 340.802813 \nL 398.56375 35.882812 \nL 41.44375 35.882812 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 86.08375 340.802813 \nL 86.08375 35.882812 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- das -->\n      <g style=\"fill: #262626\" transform=\"translate(88.669688 28.882812) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-64\" d=\"M 2566 544 \nQ 2409 219 2151 78 \nQ 1894 -63 1513 -63 \nQ 872 -63 570 368 \nQ 269 800 269 1675 \nQ 269 3444 1513 3444 \nQ 1897 3444 2153 3303 \nQ 2409 3163 2566 2856 \nL 2572 2856 \nQ 2572 2888 2570 2955 \nQ 2569 3022 2567 3095 \nQ 2566 3169 2566 3234 \nQ 2566 3300 2566 3328 \nL 2566 4638 \nL 3128 4638 \nL 3128 697 \nQ 3128 575 3129 462 \nQ 3131 350 3134 256 \nQ 3138 163 3141 95 \nQ 3144 28 3147 0 \nL 2609 0 \nQ 2603 31 2598 89 \nQ 2594 147 2589 222 \nQ 2584 297 2581 380 \nQ 2578 463 2578 544 \nL 2566 544 \nz\nM 859 1694 \nQ 859 1344 903 1094 \nQ 947 844 1044 683 \nQ 1141 522 1291 447 \nQ 1441 372 1656 372 \nQ 1878 372 2048 444 \nQ 2219 516 2333 677 \nQ 2447 838 2506 1097 \nQ 2566 1356 2566 1731 \nQ 2566 2091 2506 2339 \nQ 2447 2588 2331 2741 \nQ 2216 2894 2048 2961 \nQ 1881 3028 1663 3028 \nQ 1456 3028 1306 2956 \nQ 1156 2884 1056 2725 \nQ 956 2566 907 2311 \nQ 859 2056 859 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-64\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"111.230469\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 175.36375 340.802813 \nL 175.36375 35.882812 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- ist -->\n      <g style=\"fill: #262626\" transform=\"translate(177.949688 28.882812) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-74\" d=\"M 1731 25 \nQ 1603 -9 1470 -29 \nQ 1338 -50 1163 -50 \nQ 488 -50 488 716 \nL 488 2972 \nL 97 2972 \nL 97 3381 \nL 509 3381 \nL 675 4138 \nL 1050 4138 \nL 1050 3381 \nL 1675 3381 \nL 1675 2972 \nL 1050 2972 \nL 1050 838 \nQ 1050 594 1129 495 \nQ 1209 397 1406 397 \nQ 1488 397 1564 409 \nQ 1641 422 1731 441 \nL 1731 25 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"72.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 264.64375 340.802813 \nL 264.64375 35.882812 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- ein -->\n      <g style=\"fill: #262626\" transform=\"translate(267.229688 28.882812) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-65\" d=\"M 863 1572 \nQ 863 1306 917 1082 \nQ 972 859 1086 698 \nQ 1200 538 1378 448 \nQ 1556 359 1806 359 \nQ 2172 359 2392 506 \nQ 2613 653 2691 878 \nL 3184 738 \nQ 3131 597 3036 455 \nQ 2941 313 2781 198 \nQ 2622 84 2383 10 \nQ 2144 -63 1806 -63 \nQ 1056 -63 664 384 \nQ 272 831 272 1713 \nQ 272 2188 390 2517 \nQ 509 2847 715 3053 \nQ 922 3259 1197 3351 \nQ 1472 3444 1784 3444 \nQ 2209 3444 2495 3306 \nQ 2781 3169 2954 2926 \nQ 3128 2684 3201 2356 \nQ 3275 2028 3275 1647 \nL 3275 1572 \nL 863 1572 \nz\nM 2694 2003 \nQ 2647 2538 2422 2783 \nQ 2197 3028 1775 3028 \nQ 1634 3028 1479 2983 \nQ 1325 2938 1194 2822 \nQ 1063 2706 972 2507 \nQ 881 2309 869 2003 \nL 2694 2003 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-65\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"77.832031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 353.92375 340.802813 \nL 353.92375 35.882812 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- haus -->\n      <g style=\"fill: #262626\" transform=\"translate(356.509688 28.882812) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-68\" d=\"M 991 2803 \nQ 1084 2975 1193 3095 \nQ 1303 3216 1434 3294 \nQ 1566 3372 1722 3408 \nQ 1878 3444 2072 3444 \nQ 2397 3444 2605 3356 \nQ 2813 3269 2933 3111 \nQ 3053 2953 3098 2734 \nQ 3144 2516 3144 2253 \nL 3144 0 \nL 2578 0 \nL 2578 2144 \nQ 2578 2359 2551 2521 \nQ 2525 2684 2450 2792 \nQ 2375 2900 2237 2954 \nQ 2100 3009 1881 3009 \nQ 1681 3009 1520 2937 \nQ 1359 2866 1245 2734 \nQ 1131 2603 1068 2415 \nQ 1006 2228 1006 1994 \nL 1006 0 \nL 444 0 \nL 444 4638 \nL 1006 4638 \nL 1006 3431 \nQ 1006 3328 1003 3225 \nQ 1000 3122 995 3034 \nQ 991 2947 987 2886 \nQ 984 2825 981 2803 \nL 991 2803 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-75\" d=\"M 981 3381 \nL 981 1238 \nQ 981 991 1017 825 \nQ 1053 659 1134 557 \nQ 1216 456 1348 414 \nQ 1481 372 1678 372 \nQ 1878 372 2039 442 \nQ 2200 513 2314 645 \nQ 2428 778 2490 973 \nQ 2553 1169 2553 1422 \nL 2553 3381 \nL 3116 3381 \nL 3116 722 \nQ 3116 616 3117 498 \nQ 3119 381 3122 276 \nQ 3125 172 3128 97 \nQ 3131 22 3134 0 \nL 2603 0 \nQ 2600 16 2597 84 \nQ 2594 153 2589 242 \nQ 2584 331 2581 423 \nQ 2578 516 2578 578 \nL 2569 578 \nQ 2488 431 2391 312 \nQ 2294 194 2166 111 \nQ 2038 28 1872 -17 \nQ 1706 -63 1488 -63 \nQ 1206 -63 1003 6 \nQ 800 75 669 219 \nQ 538 363 477 588 \nQ 416 813 416 1128 \nL 416 3381 \nL 981 3381 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"166.845703\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 41.44375 54.940313 \nL 398.56375 54.940313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- this -->\n      <g style=\"fill: #262626\" transform=\"translate(18.882813 58.56375) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-68\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"83.398438\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"105.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 41.44375 93.055312 \nL 398.56375 93.055312 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- is -->\n      <g style=\"fill: #262626\" transform=\"translate(27.221875 96.67875) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 41.44375 131.170313 \nL 398.56375 131.170313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- a -->\n      <g style=\"fill: #262626\" transform=\"translate(28.882813 134.79375) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-61\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_15\">\n      <path d=\"M 41.44375 169.285313 \nL 398.56375 169.285313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- house -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 172.90875) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6f\" d=\"M 3291 1694 \nQ 3291 806 2900 371 \nQ 2509 -63 1766 -63 \nQ 1413 -63 1134 43 \nQ 856 150 664 369 \nQ 472 588 370 917 \nQ 269 1247 269 1694 \nQ 269 3444 1784 3444 \nQ 2178 3444 2464 3334 \nQ 2750 3225 2933 3006 \nQ 3116 2788 3203 2459 \nQ 3291 2131 3291 1694 \nz\nM 2700 1694 \nQ 2700 2088 2639 2344 \nQ 2578 2600 2461 2753 \nQ 2344 2906 2175 2967 \nQ 2006 3028 1794 3028 \nQ 1578 3028 1404 2964 \nQ 1231 2900 1109 2745 \nQ 988 2591 923 2334 \nQ 859 2078 859 1694 \nQ 859 1300 928 1042 \nQ 997 784 1117 631 \nQ 1238 478 1402 415 \nQ 1566 353 1759 353 \nQ 1975 353 2150 414 \nQ 2325 475 2447 628 \nQ 2569 781 2634 1040 \nQ 2700 1300 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"166.845703\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"216.845703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_17\">\n      <path d=\"M 41.44375 207.400313 \nL 398.56375 207.400313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- house -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 211.02375) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"166.845703\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"216.845703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <path d=\"M 41.44375 245.515313 \nL 398.56375 245.515313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- house -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 249.13875) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"166.845703\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"216.845703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_21\">\n      <path d=\"M 41.44375 283.630313 \nL 398.56375 283.630313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_11\">\n      <!-- a -->\n      <g style=\"fill: #262626\" transform=\"translate(28.882813 287.25375) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-61\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_23\">\n      <path d=\"M 41.44375 321.745313 \nL 398.56375 321.745313 \n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_24\"/>\n     <g id=\"text_12\">\n      <!-- house -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 325.36875) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"166.845703\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"216.845703\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 41.44375 35.882812 \nL 41.44375 73.997813 \nL 130.72375 73.997813 \nL 130.72375 35.882812 \nL 41.44375 35.882812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #1c6ab0\"/>\n    <path d=\"M 130.72375 35.882812 \nL 130.72375 73.997813 \nL 220.00375 73.997813 \nL 220.00375 35.882812 \nL 130.72375 35.882812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #58a1cf\"/>\n    <path d=\"M 220.00375 35.882812 \nL 220.00375 73.997813 \nL 309.28375 73.997813 \nL 309.28375 35.882812 \nL 220.00375 35.882812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #3686c0\"/>\n    <path d=\"M 309.28375 35.882812 \nL 309.28375 73.997813 \nL 398.56375 73.997813 \nL 398.56375 35.882812 \nL 309.28375 35.882812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083370\"/>\n    <path d=\"M 41.44375 73.997813 \nL 41.44375 112.112813 \nL 130.72375 112.112813 \nL 130.72375 73.997813 \nL 41.44375 73.997813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #105ba4\"/>\n    <path d=\"M 130.72375 73.997813 \nL 130.72375 112.112813 \nL 220.00375 112.112813 \nL 220.00375 73.997813 \nL 130.72375 73.997813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #82bbdb\"/>\n    <path d=\"M 220.00375 73.997813 \nL 220.00375 112.112813 \nL 309.28375 112.112813 \nL 309.28375 73.997813 \nL 220.00375 73.997813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #115ca5\"/>\n    <path d=\"M 309.28375 73.997813 \nL 309.28375 112.112813 \nL 398.56375 112.112813 \nL 398.56375 73.997813 \nL 309.28375 73.997813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #084f99\"/>\n    <path d=\"M 41.44375 112.112813 \nL 41.44375 150.227812 \nL 130.72375 150.227812 \nL 130.72375 112.112813 \nL 41.44375 112.112813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083674\"/>\n    <path d=\"M 130.72375 112.112813 \nL 130.72375 150.227812 \nL 220.00375 150.227812 \nL 220.00375 112.112813 \nL 130.72375 112.112813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #084387\"/>\n    <path d=\"M 220.00375 112.112813 \nL 220.00375 150.227812 \nL 309.28375 150.227812 \nL 309.28375 112.112813 \nL 220.00375 112.112813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #79b5d9\"/>\n    <path d=\"M 309.28375 112.112813 \nL 309.28375 150.227812 \nL 398.56375 150.227812 \nL 398.56375 112.112813 \nL 309.28375 112.112813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #4292c6\"/>\n    <path d=\"M 41.44375 150.227812 \nL 41.44375 188.342813 \nL 130.72375 188.342813 \nL 130.72375 150.227812 \nL 41.44375 150.227812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 130.72375 150.227812 \nL 130.72375 188.342813 \nL 220.00375 188.342813 \nL 220.00375 150.227812 \nL 130.72375 150.227812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 220.00375 150.227812 \nL 220.00375 188.342813 \nL 309.28375 188.342813 \nL 309.28375 150.227812 \nL 220.00375 150.227812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #08326e\"/>\n    <path d=\"M 309.28375 150.227812 \nL 309.28375 188.342813 \nL 398.56375 188.342813 \nL 398.56375 150.227812 \nL 309.28375 150.227812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #f7fbff\"/>\n    <path d=\"M 41.44375 188.342813 \nL 41.44375 226.457813 \nL 130.72375 226.457813 \nL 130.72375 188.342813 \nL 41.44375 188.342813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 130.72375 188.342813 \nL 130.72375 226.457813 \nL 220.00375 226.457813 \nL 220.00375 188.342813 \nL 130.72375 188.342813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083370\"/>\n    <path d=\"M 220.00375 188.342813 \nL 220.00375 226.457813 \nL 309.28375 226.457813 \nL 309.28375 188.342813 \nL 220.00375 188.342813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083674\"/>\n    <path d=\"M 309.28375 188.342813 \nL 309.28375 226.457813 \nL 398.56375 226.457813 \nL 398.56375 188.342813 \nL 309.28375 188.342813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #f0f6fd\"/>\n    <path d=\"M 41.44375 226.457813 \nL 41.44375 264.572812 \nL 130.72375 264.572812 \nL 130.72375 226.457813 \nL 41.44375 226.457813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083573\"/>\n    <path d=\"M 130.72375 226.457813 \nL 130.72375 264.572812 \nL 220.00375 264.572812 \nL 220.00375 226.457813 \nL 130.72375 226.457813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083674\"/>\n    <path d=\"M 220.00375 226.457813 \nL 220.00375 264.572812 \nL 309.28375 264.572812 \nL 309.28375 226.457813 \nL 220.00375 226.457813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083e81\"/>\n    <path d=\"M 309.28375 226.457813 \nL 309.28375 264.572812 \nL 398.56375 264.572812 \nL 398.56375 226.457813 \nL 309.28375 226.457813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #e4eff9\"/>\n    <path d=\"M 41.44375 264.572812 \nL 41.44375 302.687813 \nL 130.72375 302.687813 \nL 130.72375 264.572812 \nL 41.44375 264.572812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083471\"/>\n    <path d=\"M 130.72375 264.572812 \nL 130.72375 302.687813 \nL 220.00375 302.687813 \nL 220.00375 264.572812 \nL 130.72375 264.572812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083877\"/>\n    <path d=\"M 220.00375 264.572812 \nL 220.00375 302.687813 \nL 309.28375 302.687813 \nL 309.28375 264.572812 \nL 220.00375 264.572812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083d7f\"/>\n    <path d=\"M 309.28375 264.572812 \nL 309.28375 302.687813 \nL 398.56375 302.687813 \nL 398.56375 264.572812 \nL 309.28375 264.572812 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #e5eff9\"/>\n    <path d=\"M 41.44375 302.687813 \nL 41.44375 340.802813 \nL 130.72375 340.802813 \nL 130.72375 302.687813 \nL 41.44375 302.687813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 130.72375 302.687813 \nL 130.72375 340.802813 \nL 220.00375 340.802813 \nL 220.00375 302.687813 \nL 130.72375 302.687813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083370\"/>\n    <path d=\"M 220.00375 302.687813 \nL 220.00375 340.802813 \nL 309.28375 340.802813 \nL 309.28375 302.687813 \nL 220.00375 302.687813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #083674\"/>\n    <path d=\"M 309.28375 302.687813 \nL 309.28375 340.802813 \nL 398.56375 340.802813 \nL 398.56375 302.687813 \nL 309.28375 302.687813 \nz\n\" clip-path=\"url(#pd8439c1b0b)\" style=\"fill: #f0f6fd\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 41.44375 340.802813 \nL 41.44375 35.882812 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 398.56375 340.802813 \nL 398.56375 35.882812 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 41.44375 340.802813 \nL 398.56375 340.802813 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 41.44375 35.882812 \nL 398.56375 35.882812 \n\" style=\"fill: none\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 420.88375 340.802813 \nL 436.12975 340.802813 \nL 436.12975 35.882812 \nL 420.88375 35.882812 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_25\"/>\n     <g id=\"text_13\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.12975 282.919914) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_26\"/>\n     <g id=\"text_14\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.12975 220.933506) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_27\"/>\n     <g id=\"text_15\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.12975 158.947099) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_28\"/>\n     <g id=\"text_16\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.12975 96.960692) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABUAAAGnCAYAAABYYhxcAAAB7klEQVR4nO2cwY0kMQwD7YYfF87mtfm/95IgH4WiAhDEUZmW1cDcfz+/fyccXzrhOee8c/N537k3npQjv1Vp/jeVy+cgJZfPQUoun4MUqVHfOA0HilOzfA5ScvkcpOTyOZWuUfmY9VGQuo1GNZK2GpVPukblQ17pODV3n1OpnFNOpSD5bqRAjXJXKueUUylIvhspUKPclco55VT6CjknvxAoTgtJv8K3aI58DlJy+ZxK16h8dKzPLZ+DlFw+Bym5fA5ScvmcSteofMz6Ckk/tXzOiXqF5z5IPgepdwur3o58N1IgQ3GbtNxQOEi9gnqQoXBMGiSfc6I29cWTliptJN0VHU/KkU+6ojHyOUg1spLkY5CSz1KcRslXnY2sa1Q+QLdpxfrWqHiU7qjtpNOxYaKQlHRM16hwtGYpSqNKs1Q+qf1EbeiNB+cdBbqiOZ/j9F8j80k5S1nUusN89kHy3UjN+vJReUiA5DdWSCD58mckp1HuVSdIvhsp0GLGjRRolgKtkBpJQX+xsxOVDv2/i7nHczdSM5R8cLo/Q8mHHCm5oXCQkhsKBym5oXCQkhsKBym5oXCQkhsKBymQoVTkH3X3QYZCkk9BqnRF52MnqjD0kq7ofHCQAhkK5kTJ5cuRAhkKyaS3mAkHBym5ofwHIaUJOtL5PEkAAAAASUVORK5CYII=\" id=\"image2381ce665e\" transform=\"scale(1 -1) translate(0 -304.56)\" x=\"421.2\" y=\"-36\" width=\"15.12\" height=\"304.56\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 420.88375 340.802813 \nL 428.50675 340.802813 \nL 436.12975 340.802813 \nL 436.12975 35.882812 \nL 428.50675 35.882812 \nL 420.88375 35.882812 \nL 420.88375 340.802813 \nz\n\" style=\"fill: none\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd8439c1b0b\">\n   <rect x=\"41.44375\" y=\"35.882812\" width=\"357.12\" height=\"304.92\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_attention(translator, 'das ist ein haus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnyodKiTy9Um"
      },
      "source": [
        "Use these heatmaps to inspect the attention patterns for selected German sentences. Try to find sentences for which the model produces reasonably good English translations. If your German is a bit rusty (or non-existent), use sentences from the validation data. It might be interesting to look at examples where the German and the English word order differ substantially. Document your exploration in a short reflection piece (ca. 150¬†words). Respond to the following prompts:\n",
        "\n",
        "* What sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\n",
        "* Based on what you know about attention, did you expect your results? Was there anything surprising in them?\n",
        "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggGiKtLPy9Um"
      },
      "source": [
        "**ü•≥ Congratulations on finishing this lab! ü•≥**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention(translator, 'mein vater ist klein')"
      ],
      "metadata": {
        "id": "pVSGygzp952j",
        "outputId": "546ae280-44b3-4664-f9c2-906cd8ed217a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-192-c887944f02de>:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
            "<ipython-input-192-c887944f02de>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(translation.split(), minor=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"464.789125pt\" height=\"348.549688pt\" viewBox=\"0 0 464.789125 348.549688\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-09T08:24:24.602806</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 348.549688 \nL 464.789125 348.549688 \nL 464.789125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.003125 341.349688 \nL 399.123125 341.349688 \nL 399.123125 36.429688 \nL 42.003125 36.429688 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 86.643125 341.349688 \nL 86.643125 36.429688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- mein -->\n      <g style=\"fill: #262626\" transform=\"translate(89.229063 29.429688) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \nL 2400 2144 \nQ 2400 2391 2369 2556 \nQ 2338 2722 2264 2823 \nQ 2191 2925 2072 2967 \nQ 1953 3009 1781 3009 \nQ 1603 3009 1459 2939 \nQ 1316 2869 1214 2736 \nQ 1113 2603 1058 2408 \nQ 1003 2213 1003 1959 \nL 1003 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1066 2950 1153 3069 \nQ 1241 3188 1358 3270 \nQ 1475 3353 1626 3398 \nQ 1778 3444 1978 3444 \nQ 2363 3444 2586 3291 \nQ 2809 3138 2897 2803 \nL 2906 2803 \nQ 2981 2950 3075 3069 \nQ 3169 3188 3294 3270 \nQ 3419 3353 3575 3398 \nQ 3731 3444 3931 3444 \nQ 4188 3444 4373 3375 \nQ 4559 3306 4678 3162 \nQ 4797 3019 4853 2792 \nQ 4909 2566 4909 2253 \nL 4909 0 \nL 4353 0 \nL 4353 2144 \nQ 4353 2391 4322 2556 \nQ 4291 2722 4217 2823 \nQ 4144 2925 4025 2967 \nQ 3906 3009 3734 3009 \nQ 3556 3009 3412 2942 \nQ 3269 2875 3167 2744 \nQ 3066 2613 3011 2416 \nQ 2956 2219 2956 1959 \nL 2956 0 \nL 2400 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-65\" d=\"M 863 1572 \nQ 863 1306 917 1082 \nQ 972 859 1086 698 \nQ 1200 538 1378 448 \nQ 1556 359 1806 359 \nQ 2172 359 2392 506 \nQ 2613 653 2691 878 \nL 3184 738 \nQ 3131 597 3036 455 \nQ 2941 313 2781 198 \nQ 2622 84 2383 10 \nQ 2144 -63 1806 -63 \nQ 1056 -63 664 384 \nQ 272 831 272 1713 \nQ 272 2188 390 2517 \nQ 509 2847 715 3053 \nQ 922 3259 1197 3351 \nQ 1472 3444 1784 3444 \nQ 2209 3444 2495 3306 \nQ 2781 3169 2954 2926 \nQ 3128 2684 3201 2356 \nQ 3275 2028 3275 1647 \nL 3275 1572 \nL 863 1572 \nz\nM 2694 2003 \nQ 2647 2538 2422 2783 \nQ 2197 3028 1775 3028 \nQ 1634 3028 1479 2983 \nQ 1325 2938 1194 2822 \nQ 1063 2706 972 2507 \nQ 881 2309 869 2003 \nL 2694 2003 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"138.916016\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"161.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 175.923125 341.349688 \nL 175.923125 36.429688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- vater -->\n      <g style=\"fill: #262626\" transform=\"translate(178.509063 29.429688) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-76\" d=\"M 1916 0 \nL 1250 0 \nL 22 3381 \nL 622 3381 \nL 1366 1181 \nQ 1388 1113 1417 1014 \nQ 1447 916 1478 809 \nQ 1509 703 1536 604 \nQ 1563 506 1581 441 \nQ 1600 506 1629 604 \nQ 1659 703 1690 806 \nQ 1722 909 1755 1007 \nQ 1788 1106 1813 1175 \nL 2581 3381 \nL 3178 3381 \nL 1916 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-74\" d=\"M 1731 25 \nQ 1603 -9 1470 -29 \nQ 1338 -50 1163 -50 \nQ 488 -50 488 716 \nL 488 2972 \nL 97 2972 \nL 97 3381 \nL 509 3381 \nL 675 4138 \nL 1050 4138 \nL 1050 3381 \nL 1675 3381 \nL 1675 2972 \nL 1050 2972 \nL 1050 838 \nQ 1050 594 1129 495 \nQ 1209 397 1406 397 \nQ 1488 397 1564 409 \nQ 1641 422 1731 441 \nL 1731 25 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-72\" d=\"M 444 0 \nL 444 2594 \nQ 444 2700 442 2811 \nQ 441 2922 437 3025 \nQ 434 3128 431 3218 \nQ 428 3309 425 3381 \nL 956 3381 \nQ 959 3309 964 3217 \nQ 969 3125 973 3028 \nQ 978 2931 979 2842 \nQ 981 2753 981 2691 \nL 994 2691 \nQ 1053 2884 1120 3026 \nQ 1188 3169 1278 3261 \nQ 1369 3353 1494 3398 \nQ 1619 3444 1797 3444 \nQ 1866 3444 1928 3433 \nQ 1991 3422 2025 3413 \nL 2025 2897 \nQ 1969 2913 1894 2920 \nQ 1819 2928 1725 2928 \nQ 1531 2928 1395 2840 \nQ 1259 2753 1173 2598 \nQ 1088 2444 1047 2230 \nQ 1006 2016 1006 1763 \nL 1006 0 \nL 444 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-76\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"105.615234\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"133.398438\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"189.013672\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 265.203125 341.349688 \nL 265.203125 36.429688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- ist -->\n      <g style=\"fill: #262626\" transform=\"translate(267.789063 29.429688) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"72.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 354.483125 341.349688 \nL 354.483125 36.429688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- klein -->\n      <g style=\"fill: #262626\" transform=\"translate(357.069063 29.429688) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6b\" d=\"M 2550 0 \nL 1406 1544 \nL 994 1203 \nL 994 0 \nL 431 0 \nL 431 4638 \nL 994 4638 \nL 994 1741 \nL 2478 3381 \nL 3138 3381 \nL 1766 1928 \nL 3209 0 \nL 2550 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6c\" d=\"M 431 0 \nL 431 4638 \nL 994 4638 \nL 994 0 \nL 431 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6b\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"72.216797\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"127.832031\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"150.048828\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 42.003125 61.839687 \nL 399.123125 61.839687 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- my -->\n      <g style=\"fill: #262626\" transform=\"translate(21.673438 65.463125) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-79\" d=\"M 1888 0 \nQ 1769 -306 1645 -551 \nQ 1522 -797 1369 -970 \nQ 1216 -1144 1028 -1236 \nQ 841 -1328 597 -1328 \nQ 491 -1328 400 -1322 \nQ 309 -1316 209 -1294 \nL 209 -872 \nQ 269 -881 344 -886 \nQ 419 -891 472 -891 \nQ 719 -891 931 -706 \nQ 1144 -522 1303 -119 \nL 1356 16 \nL 16 3381 \nL 616 3381 \nL 1328 1513 \nQ 1359 1428 1407 1287 \nQ 1456 1147 1504 1006 \nQ 1553 866 1590 753 \nQ 1628 641 1634 613 \nQ 1644 647 1680 748 \nQ 1716 850 1761 975 \nQ 1806 1100 1853 1228 \nQ 1900 1356 1931 1450 \nL 2594 3381 \nL 3188 3381 \nL 1888 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-79\" x=\"83.300781\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 42.003125 112.659688 \nL 399.123125 112.659688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- father -->\n      <g style=\"fill: #262626\" transform=\"translate(9.434375 116.283125) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-66\" d=\"M 1128 2972 \nL 1128 0 \nL 566 0 \nL 566 2972 \nL 91 2972 \nL 91 3381 \nL 566 3381 \nL 566 3763 \nQ 566 3947 600 4105 \nQ 634 4263 726 4380 \nQ 819 4497 978 4564 \nQ 1138 4631 1391 4631 \nQ 1491 4631 1598 4622 \nQ 1706 4613 1788 4594 \nL 1788 4166 \nQ 1734 4175 1664 4183 \nQ 1594 4191 1538 4191 \nQ 1413 4191 1333 4156 \nQ 1253 4122 1208 4058 \nQ 1163 3994 1145 3900 \nQ 1128 3806 1128 3684 \nL 1128 3381 \nL 1788 3381 \nL 1788 2972 \nL 1128 2972 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-68\" d=\"M 991 2803 \nQ 1084 2975 1193 3095 \nQ 1303 3216 1434 3294 \nQ 1566 3372 1722 3408 \nQ 1878 3444 2072 3444 \nQ 2397 3444 2605 3356 \nQ 2813 3269 2933 3111 \nQ 3053 2953 3098 2734 \nQ 3144 2516 3144 2253 \nL 3144 0 \nL 2578 0 \nL 2578 2144 \nQ 2578 2359 2551 2521 \nQ 2525 2684 2450 2792 \nQ 2375 2900 2237 2954 \nQ 2100 3009 1881 3009 \nQ 1681 3009 1520 2937 \nQ 1359 2866 1245 2734 \nQ 1131 2603 1068 2415 \nQ 1006 2228 1006 1994 \nL 1006 0 \nL 444 0 \nL 444 4638 \nL 1006 4638 \nL 1006 3431 \nQ 1006 3328 1003 3225 \nQ 1000 3122 995 3034 \nQ 991 2947 987 2886 \nQ 984 2825 981 2803 \nL 991 2803 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-66\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"83.398438\"/>\n       <use xlink:href=\"#LiberationSans-68\" x=\"111.181641\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"166.796875\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"222.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 42.003125 163.479688 \nL 399.123125 163.479688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- is -->\n      <g style=\"fill: #262626\" transform=\"translate(27.78125 167.103125) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_15\">\n      <path d=\"M 42.003125 214.299688 \nL 399.123125 214.299688 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- small -->\n      <g style=\"fill: #262626\" transform=\"translate(11.66875 217.923125) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-73\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"133.300781\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"211.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_17\">\n      <path d=\"M 42.003125 265.119687 \nL 399.123125 265.119687 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- &lt;unk&gt; -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 268.743125) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-3c\" d=\"M 316 1784 \nL 316 2425 \nL 3425 3731 \nL 3425 3250 \nL 744 2106 \nL 3425 959 \nL 3425 481 \nL 316 1784 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-75\" d=\"M 981 3381 \nL 981 1238 \nQ 981 991 1017 825 \nQ 1053 659 1134 557 \nQ 1216 456 1348 414 \nQ 1481 372 1678 372 \nQ 1878 372 2039 442 \nQ 2200 513 2314 645 \nQ 2428 778 2490 973 \nQ 2553 1169 2553 1422 \nL 2553 3381 \nL 3116 3381 \nL 3116 722 \nQ 3116 616 3117 498 \nQ 3119 381 3122 276 \nQ 3125 172 3128 97 \nQ 3131 22 3134 0 \nL 2603 0 \nQ 2600 16 2597 84 \nQ 2594 153 2589 242 \nQ 2584 331 2581 423 \nQ 2578 516 2578 578 \nL 2569 578 \nQ 2488 431 2391 312 \nQ 2294 194 2166 111 \nQ 2038 28 1872 -17 \nQ 1706 -63 1488 -63 \nQ 1206 -63 1003 6 \nQ 800 75 669 219 \nQ 538 363 477 588 \nQ 416 813 416 1128 \nL 416 3381 \nL 981 3381 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-3e\" d=\"M 316 481 \nL 316 959 \nL 2997 2106 \nL 316 3250 \nL 316 3731 \nL 3425 2425 \nL 3425 1784 \nL 316 481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-3c\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"58.398438\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"114.013672\"/>\n       <use xlink:href=\"#LiberationSans-6b\" x=\"169.628906\"/>\n       <use xlink:href=\"#LiberationSans-3e\" x=\"219.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <path d=\"M 42.003125 315.939687 \nL 399.123125 315.939687 \n\" clip-path=\"url(#p744ab07224)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(32.225 319.563125) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 42.003125 36.429688 \nL 42.003125 87.249687 \nL 131.283125 87.249687 \nL 131.283125 36.429688 \nL 42.003125 36.429688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #3c8cc3\"/>\n    <path d=\"M 131.283125 36.429688 \nL 131.283125 87.249687 \nL 220.563125 87.249687 \nL 220.563125 36.429688 \nL 131.283125 36.429688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #2c7cba\"/>\n    <path d=\"M 220.563125 36.429688 \nL 220.563125 87.249687 \nL 309.843125 87.249687 \nL 309.843125 36.429688 \nL 220.563125 36.429688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #1f6eb3\"/>\n    <path d=\"M 309.843125 36.429688 \nL 309.843125 87.249687 \nL 399.123125 87.249687 \nL 399.123125 36.429688 \nL 309.843125 36.429688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #1663aa\"/>\n    <path d=\"M 42.003125 87.249687 \nL 42.003125 138.069687 \nL 131.283125 138.069687 \nL 131.283125 87.249687 \nL 42.003125 87.249687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #2171b5\"/>\n    <path d=\"M 131.283125 87.249687 \nL 131.283125 138.069687 \nL 220.563125 138.069687 \nL 220.563125 87.249687 \nL 131.283125 87.249687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #a3cce3\"/>\n    <path d=\"M 220.563125 87.249687 \nL 220.563125 138.069687 \nL 309.843125 138.069687 \nL 309.843125 87.249687 \nL 220.563125 87.249687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #083c7d\"/>\n    <path d=\"M 309.843125 87.249687 \nL 309.843125 138.069687 \nL 399.123125 138.069687 \nL 399.123125 87.249687 \nL 309.843125 87.249687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #0b559f\"/>\n    <path d=\"M 42.003125 138.069687 \nL 42.003125 188.889688 \nL 131.283125 188.889688 \nL 131.283125 138.069687 \nL 42.003125 138.069687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #084184\"/>\n    <path d=\"M 131.283125 138.069687 \nL 131.283125 188.889688 \nL 220.563125 188.889688 \nL 220.563125 138.069687 \nL 131.283125 138.069687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #1562a9\"/>\n    <path d=\"M 220.563125 138.069687 \nL 220.563125 188.889688 \nL 309.843125 188.889688 \nL 309.843125 138.069687 \nL 220.563125 138.069687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #4b98ca\"/>\n    <path d=\"M 309.843125 138.069687 \nL 309.843125 188.889688 \nL 399.123125 188.889688 \nL 399.123125 138.069687 \nL 309.843125 138.069687 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #519ccc\"/>\n    <path d=\"M 42.003125 188.889688 \nL 42.003125 239.709688 \nL 131.283125 239.709688 \nL 131.283125 188.889688 \nL 42.003125 188.889688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #08316d\"/>\n    <path d=\"M 131.283125 188.889688 \nL 131.283125 239.709688 \nL 220.563125 239.709688 \nL 220.563125 188.889688 \nL 131.283125 188.889688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #08326e\"/>\n    <path d=\"M 220.563125 188.889688 \nL 220.563125 239.709688 \nL 309.843125 239.709688 \nL 309.843125 188.889688 \nL 220.563125 188.889688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #084285\"/>\n    <path d=\"M 309.843125 188.889688 \nL 309.843125 239.709688 \nL 399.123125 239.709688 \nL 399.123125 188.889688 \nL 309.843125 188.889688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #f7fbff\"/>\n    <path d=\"M 42.003125 239.709688 \nL 42.003125 290.529688 \nL 131.283125 290.529688 \nL 131.283125 239.709688 \nL 42.003125 239.709688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #08306b\"/>\n    <path d=\"M 131.283125 239.709688 \nL 131.283125 290.529688 \nL 220.563125 290.529688 \nL 220.563125 239.709688 \nL 131.283125 239.709688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #083c7d\"/>\n    <path d=\"M 220.563125 239.709688 \nL 220.563125 290.529688 \nL 309.843125 290.529688 \nL 309.843125 239.709688 \nL 220.563125 239.709688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #083877\"/>\n    <path d=\"M 309.843125 239.709688 \nL 309.843125 290.529688 \nL 399.123125 290.529688 \nL 399.123125 239.709688 \nL 309.843125 239.709688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #f7fbff\"/>\n    <path d=\"M 42.003125 290.529688 \nL 42.003125 341.349688 \nL 131.283125 341.349688 \nL 131.283125 290.529688 \nL 42.003125 290.529688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #083471\"/>\n    <path d=\"M 131.283125 290.529688 \nL 131.283125 341.349688 \nL 220.563125 341.349688 \nL 220.563125 290.529688 \nL 131.283125 290.529688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #0a539e\"/>\n    <path d=\"M 220.563125 290.529688 \nL 220.563125 341.349688 \nL 309.843125 341.349688 \nL 309.843125 290.529688 \nL 220.563125 290.529688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #084488\"/>\n    <path d=\"M 309.843125 290.529688 \nL 309.843125 341.349688 \nL 399.123125 341.349688 \nL 399.123125 290.529688 \nL 309.843125 290.529688 \nz\n\" clip-path=\"url(#p744ab07224)\" style=\"fill: #dbe9f6\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.003125 341.349688 \nL 42.003125 36.429688 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 399.123125 341.349688 \nL 399.123125 36.429688 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.003125 341.349688 \nL 399.123125 341.349688 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.003125 36.429688 \nL 399.123125 36.429688 \n\" style=\"fill: none\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 421.443125 341.349688 \nL 436.689125 341.349688 \nL 436.689125 36.429688 \nL 421.443125 36.429688 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_21\"/>\n     <g id=\"text_11\">\n      <!-- 0.1 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 313.946404) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-31\" d=\"M 488 0 \nL 488 478 \nL 1609 478 \nL 1609 3866 \nL 616 3156 \nL 616 3688 \nL 1656 4403 \nL 2175 4403 \nL 2175 478 \nL 3247 478 \nL 3247 0 \nL 488 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-31\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_22\"/>\n     <g id=\"text_12\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 279.669988) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_23\"/>\n     <g id=\"text_13\">\n      <!-- 0.3 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 245.393572) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-33\" d=\"M 3278 1216 \nQ 3278 913 3179 675 \nQ 3081 438 2892 273 \nQ 2703 109 2423 23 \nQ 2144 -63 1784 -63 \nQ 1375 -63 1095 39 \nQ 816 141 634 308 \nQ 453 475 362 689 \nQ 272 903 244 1131 \nL 825 1184 \nQ 850 1009 917 865 \nQ 984 722 1100 619 \nQ 1216 516 1384 459 \nQ 1553 403 1784 403 \nQ 2209 403 2451 612 \nQ 2694 822 2694 1234 \nQ 2694 1478 2586 1626 \nQ 2478 1775 2317 1858 \nQ 2156 1941 1967 1969 \nQ 1778 1997 1619 1997 \nL 1300 1997 \nL 1300 2484 \nL 1606 2484 \nQ 1766 2484 1937 2517 \nQ 2109 2550 2251 2636 \nQ 2394 2722 2486 2869 \nQ 2578 3016 2578 3244 \nQ 2578 3597 2370 3801 \nQ 2163 4006 1753 4006 \nQ 1381 4006 1151 3815 \nQ 922 3625 884 3278 \nL 319 3322 \nQ 353 3613 478 3828 \nQ 603 4044 795 4186 \nQ 988 4328 1234 4398 \nQ 1481 4469 1759 4469 \nQ 2128 4469 2393 4376 \nQ 2659 4284 2828 4126 \nQ 2997 3969 3076 3756 \nQ 3156 3544 3156 3303 \nQ 3156 3109 3103 2940 \nQ 3050 2772 2937 2637 \nQ 2825 2503 2651 2406 \nQ 2478 2309 2234 2259 \nL 2234 2247 \nQ 2500 2219 2697 2126 \nQ 2894 2034 3022 1896 \nQ 3150 1759 3214 1584 \nQ 3278 1409 3278 1216 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-33\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_24\"/>\n     <g id=\"text_14\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 211.117156) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_25\"/>\n     <g id=\"text_15\">\n      <!-- 0.5 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 176.840741) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-35\" d=\"M 3291 1434 \nQ 3291 1103 3191 828 \nQ 3091 553 2894 354 \nQ 2697 156 2405 46 \nQ 2113 -63 1728 -63 \nQ 1381 -63 1123 18 \nQ 866 100 687 242 \nQ 509 384 404 575 \nQ 300 766 256 984 \nL 825 1050 \nQ 859 925 921 808 \nQ 984 691 1092 598 \nQ 1200 506 1358 451 \nQ 1516 397 1741 397 \nQ 1959 397 2137 464 \nQ 2316 531 2442 662 \nQ 2569 794 2637 984 \nQ 2706 1175 2706 1422 \nQ 2706 1625 2640 1795 \nQ 2575 1966 2453 2089 \nQ 2331 2213 2154 2281 \nQ 1978 2350 1753 2350 \nQ 1613 2350 1494 2325 \nQ 1375 2300 1273 2256 \nQ 1172 2213 1089 2155 \nQ 1006 2097 934 2034 \nL 384 2034 \nL 531 4403 \nL 3034 4403 \nL 3034 3925 \nL 1044 3925 \nL 959 2528 \nQ 1109 2644 1334 2726 \nQ 1559 2809 1869 2809 \nQ 2197 2809 2459 2709 \nQ 2722 2609 2906 2426 \nQ 3091 2244 3191 1991 \nQ 3291 1738 3291 1434 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-35\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_26\"/>\n     <g id=\"text_16\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 142.564325) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_27\"/>\n     <g id=\"text_17\">\n      <!-- 0.7 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 108.287909) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-37\" d=\"M 3238 3947 \nQ 2906 3441 2628 2973 \nQ 2350 2506 2150 2032 \nQ 1950 1559 1839 1061 \nQ 1728 563 1728 0 \nL 1141 0 \nQ 1141 528 1266 1036 \nQ 1391 1544 1603 2033 \nQ 1816 2522 2097 2994 \nQ 2378 3466 2694 3925 \nL 328 3925 \nL 328 4403 \nL 3238 4403 \nL 3238 3947 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-37\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_28\"/>\n     <g id=\"text_18\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 74.011494) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABYAAAGnCAYAAACzVadfAAACF0lEQVR4nO2cu63EMBADJUPBK+f6uv7je4Etyg0wGGCmAGLBpVYfA55/n+9vFLgaomOMscbsaK8xZ0WYZ0Wz4o7HWhF4cdOKwIubVgRe3IjNu8zxjTl+C2vFDS9uWhF4cdOKwKvY5gXerNCK0Kt4tprXEm42ryNs8wLPCl7F5rgvzLOCV7E5DryKgVYYt7owzwpexeY48CoGWmHc6sI8K3gVm+PAq3iVdLXiwLOiWXFJ+Cp9++dZwYubVgRexTYv8GaFVgRe3LQi8OKmFYEXN60IvIptXuDNCq04wpdWPPBW3io9VwCt4MVtzdKTd88K4/YSruhWm0eLm9v/xrgd4ZITwCHEG/RAK3grz9Pmhhi3lrDb/0tYK26I2z/OCl7cWspEK3Bx8+y2ATbPZ95NbQjZvMCzgjcrbF4ANs83+g2vYg8sR5i4pG3eIwxsHm5sFs9uHWFXXvDgHZCzoiTM2/55nzb9yrsBrrzeh8KKLNEK3qwAWmHcjnBF1rH5ona5AVrReh4DWuGVty7MyzHxsQlnhXGLMO+xybgd4Y4u8PYPvDUBf1/lyotwR9e//h28KgTjdoQdQke4owtMhUMoOISCQygYtyPsENrw4uYQCry4OYQCL24OocCLm0Mo8OIGHEI1K4apeAAOIaIVtLgVt/8OrrzQO3gTt/8OvLgBhxBu5WlFMG4v4Y4ucPvnxQ258jrw4uYQCv/ARQk69n7x0gAAAABJRU5ErkJggg==\" id=\"imageab09f5b89a\" transform=\"scale(1 -1) translate(0 -304.56)\" x=\"421.2\" y=\"-36.72\" width=\"15.84\" height=\"304.56\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 421.443125 341.349688 \nL 429.066125 341.349688 \nL 436.689125 341.349688 \nL 436.689125 36.429688 \nL 429.066125 36.429688 \nL 421.443125 36.429688 \nL 421.443125 341.349688 \nz\n\" style=\"fill: none\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p744ab07224\">\n   <rect x=\"42.003125\" y=\"36.429688\" width=\"357.12\" height=\"304.92\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention(translator, 'die katze ist nicht klein')"
      ],
      "metadata": {
        "id": "okRspTGz9-8_",
        "outputId": "24ac0bb7-41b1-49ba-a7a7-9a60a5afec07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-192-c887944f02de>:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
            "<ipython-input-192-c887944f02de>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(translation.split(), minor=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"464.789125pt\" height=\"350.22pt\" viewBox=\"0 0 464.789125 350.22\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-09T08:24:53.730279</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 350.22 \nL 464.789125 350.22 \nL 464.789125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.003125 343.02 \nL 399.123125 343.02 \nL 399.123125 38.1 \nL 42.003125 38.1 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 77.715125 343.02 \nL 77.715125 38.1 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- die -->\n      <g style=\"fill: #262626\" transform=\"translate(80.301063 31.1) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-64\" d=\"M 2566 544 \nQ 2409 219 2151 78 \nQ 1894 -63 1513 -63 \nQ 872 -63 570 368 \nQ 269 800 269 1675 \nQ 269 3444 1513 3444 \nQ 1897 3444 2153 3303 \nQ 2409 3163 2566 2856 \nL 2572 2856 \nQ 2572 2888 2570 2955 \nQ 2569 3022 2567 3095 \nQ 2566 3169 2566 3234 \nQ 2566 3300 2566 3328 \nL 2566 4638 \nL 3128 4638 \nL 3128 697 \nQ 3128 575 3129 462 \nQ 3131 350 3134 256 \nQ 3138 163 3141 95 \nQ 3144 28 3147 0 \nL 2609 0 \nQ 2603 31 2598 89 \nQ 2594 147 2589 222 \nQ 2584 297 2581 380 \nQ 2578 463 2578 544 \nL 2566 544 \nz\nM 859 1694 \nQ 859 1344 903 1094 \nQ 947 844 1044 683 \nQ 1141 522 1291 447 \nQ 1441 372 1656 372 \nQ 1878 372 2048 444 \nQ 2219 516 2333 677 \nQ 2447 838 2506 1097 \nQ 2566 1356 2566 1731 \nQ 2566 2091 2506 2339 \nQ 2447 2588 2331 2741 \nQ 2216 2894 2048 2961 \nQ 1881 3028 1663 3028 \nQ 1456 3028 1306 2956 \nQ 1156 2884 1056 2725 \nQ 956 2566 907 2311 \nQ 859 2056 859 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-65\" d=\"M 863 1572 \nQ 863 1306 917 1082 \nQ 972 859 1086 698 \nQ 1200 538 1378 448 \nQ 1556 359 1806 359 \nQ 2172 359 2392 506 \nQ 2613 653 2691 878 \nL 3184 738 \nQ 3131 597 3036 455 \nQ 2941 313 2781 198 \nQ 2622 84 2383 10 \nQ 2144 -63 1806 -63 \nQ 1056 -63 664 384 \nQ 272 831 272 1713 \nQ 272 2188 390 2517 \nQ 509 2847 715 3053 \nQ 922 3259 1197 3351 \nQ 1472 3444 1784 3444 \nQ 2209 3444 2495 3306 \nQ 2781 3169 2954 2926 \nQ 3128 2684 3201 2356 \nQ 3275 2028 3275 1647 \nL 3275 1572 \nL 863 1572 \nz\nM 2694 2003 \nQ 2647 2538 2422 2783 \nQ 2197 3028 1775 3028 \nQ 1634 3028 1479 2983 \nQ 1325 2938 1194 2822 \nQ 1063 2706 972 2507 \nQ 881 2309 869 2003 \nL 2694 2003 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-64\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"77.832031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 149.139125 343.02 \nL 149.139125 38.1 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- katze -->\n      <g style=\"fill: #262626\" transform=\"translate(151.725063 31.1) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6b\" d=\"M 2550 0 \nL 1406 1544 \nL 994 1203 \nL 994 0 \nL 431 0 \nL 431 4638 \nL 994 4638 \nL 994 1741 \nL 2478 3381 \nL 3138 3381 \nL 1766 1928 \nL 3209 0 \nL 2550 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-74\" d=\"M 1731 25 \nQ 1603 -9 1470 -29 \nQ 1338 -50 1163 -50 \nQ 488 -50 488 716 \nL 488 2972 \nL 97 2972 \nL 97 3381 \nL 509 3381 \nL 675 4138 \nL 1050 4138 \nL 1050 3381 \nL 1675 3381 \nL 1675 2972 \nL 1050 2972 \nL 1050 838 \nQ 1050 594 1129 495 \nQ 1209 397 1406 397 \nQ 1488 397 1564 409 \nQ 1641 422 1731 441 \nL 1731 25 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-7a\" d=\"M 153 0 \nL 153 428 \nL 2219 2947 \nL 278 2947 \nL 278 3381 \nL 2853 3381 \nL 2853 2953 \nL 784 434 \nL 2969 434 \nL 2969 0 \nL 153 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6b\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"105.615234\"/>\n       <use xlink:href=\"#LiberationSans-7a\" x=\"133.398438\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"183.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 220.563125 343.02 \nL 220.563125 38.1 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- ist -->\n      <g style=\"fill: #262626\" transform=\"translate(223.149063 31.1) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"72.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 291.987125 343.02 \nL 291.987125 38.1 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- nicht -->\n      <g style=\"fill: #262626\" transform=\"translate(294.573063 31.1) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-63\" d=\"M 859 1706 \nQ 859 1416 901 1172 \nQ 944 928 1044 751 \nQ 1144 575 1308 478 \nQ 1472 381 1713 381 \nQ 2013 381 2214 543 \nQ 2416 706 2463 1044 \nL 3031 1006 \nQ 3003 797 2909 603 \nQ 2816 409 2655 262 \nQ 2494 116 2262 26 \nQ 2031 -63 1728 -63 \nQ 1331 -63 1053 71 \nQ 775 206 601 442 \nQ 428 678 350 998 \nQ 272 1319 272 1694 \nQ 272 2034 328 2295 \nQ 384 2556 486 2751 \nQ 588 2947 725 3080 \nQ 863 3213 1023 3292 \nQ 1184 3372 1362 3408 \nQ 1541 3444 1722 3444 \nQ 2006 3444 2228 3366 \nQ 2450 3288 2611 3152 \nQ 2772 3016 2872 2831 \nQ 2972 2647 3013 2434 \nL 2434 2391 \nQ 2391 2672 2212 2837 \nQ 2034 3003 1706 3003 \nQ 1466 3003 1305 2923 \nQ 1144 2844 1044 2683 \nQ 944 2522 901 2278 \nQ 859 2034 859 1706 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-68\" d=\"M 991 2803 \nQ 1084 2975 1193 3095 \nQ 1303 3216 1434 3294 \nQ 1566 3372 1722 3408 \nQ 1878 3444 2072 3444 \nQ 2397 3444 2605 3356 \nQ 2813 3269 2933 3111 \nQ 3053 2953 3098 2734 \nQ 3144 2516 3144 2253 \nL 3144 0 \nL 2578 0 \nL 2578 2144 \nQ 2578 2359 2551 2521 \nQ 2525 2684 2450 2792 \nQ 2375 2900 2237 2954 \nQ 2100 3009 1881 3009 \nQ 1681 3009 1520 2937 \nQ 1359 2866 1245 2734 \nQ 1131 2603 1068 2415 \nQ 1006 2228 1006 1994 \nL 1006 0 \nL 444 0 \nL 444 4638 \nL 1006 4638 \nL 1006 3431 \nQ 1006 3328 1003 3225 \nQ 1000 3122 995 3034 \nQ 991 2947 987 2886 \nQ 984 2825 981 2803 \nL 991 2803 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6e\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-63\" x=\"77.832031\"/>\n       <use xlink:href=\"#LiberationSans-68\" x=\"127.832031\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"183.447266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 363.411125 343.02 \nL 363.411125 38.1 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- klein -->\n      <g style=\"fill: #262626\" transform=\"translate(365.997063 31.1) rotate(-90) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6c\" d=\"M 431 0 \nL 431 4638 \nL 994 4638 \nL 994 0 \nL 431 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6b\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"72.216797\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"127.832031\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"150.048828\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 42.003125 57.1575 \nL 399.123125 57.1575 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- the -->\n      <g style=\"fill: #262626\" transform=\"translate(21.103125 60.780937) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-68\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 42.003125 95.2725 \nL 399.123125 95.2725 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- cat -->\n      <g style=\"fill: #262626\" transform=\"translate(21.664063 98.895937) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-63\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"105.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 42.003125 133.3875 \nL 399.123125 133.3875 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- is -->\n      <g style=\"fill: #262626\" transform=\"translate(27.78125 137.010937) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 42.003125 171.5025 \nL 399.123125 171.5025 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- not -->\n      <g style=\"fill: #262626\" transform=\"translate(21.103125 175.125937) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6f\" d=\"M 3291 1694 \nQ 3291 806 2900 371 \nQ 2509 -63 1766 -63 \nQ 1413 -63 1134 43 \nQ 856 150 664 369 \nQ 472 588 370 917 \nQ 269 1247 269 1694 \nQ 269 3444 1784 3444 \nQ 2178 3444 2464 3334 \nQ 2750 3225 2933 3006 \nQ 3116 2788 3203 2459 \nQ 3291 2131 3291 1694 \nz\nM 2700 1694 \nQ 2700 2088 2639 2344 \nQ 2578 2600 2461 2753 \nQ 2344 2906 2175 2967 \nQ 2006 3028 1794 3028 \nQ 1578 3028 1404 2964 \nQ 1231 2900 1109 2745 \nQ 988 2591 923 2334 \nQ 859 2078 859 1694 \nQ 859 1300 928 1042 \nQ 997 784 1117 631 \nQ 1238 478 1402 415 \nQ 1566 353 1759 353 \nQ 1975 353 2150 414 \nQ 2325 475 2447 628 \nQ 2569 781 2634 1040 \nQ 2700 1300 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6e\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"111.230469\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 42.003125 209.6175 \nL 399.123125 209.6175 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- a -->\n      <g style=\"fill: #262626\" transform=\"translate(29.442188 213.240937) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-61\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path d=\"M 42.003125 247.7325 \nL 399.123125 247.7325 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_11\">\n      <!-- small -->\n      <g style=\"fill: #262626\" transform=\"translate(11.66875 251.355937) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \nL 2400 2144 \nQ 2400 2391 2369 2556 \nQ 2338 2722 2264 2823 \nQ 2191 2925 2072 2967 \nQ 1953 3009 1781 3009 \nQ 1603 3009 1459 2939 \nQ 1316 2869 1214 2736 \nQ 1113 2603 1058 2408 \nQ 1003 2213 1003 1959 \nL 1003 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1066 2950 1153 3069 \nQ 1241 3188 1358 3270 \nQ 1475 3353 1626 3398 \nQ 1778 3444 1978 3444 \nQ 2363 3444 2586 3291 \nQ 2809 3138 2897 2803 \nL 2906 2803 \nQ 2981 2950 3075 3069 \nQ 3169 3188 3294 3270 \nQ 3419 3353 3575 3398 \nQ 3731 3444 3931 3444 \nQ 4188 3444 4373 3375 \nQ 4559 3306 4678 3162 \nQ 4797 3019 4853 2792 \nQ 4909 2566 4909 2253 \nL 4909 0 \nL 4353 0 \nL 4353 2144 \nQ 4353 2391 4322 2556 \nQ 4291 2722 4217 2823 \nQ 4144 2925 4025 2967 \nQ 3906 3009 3734 3009 \nQ 3556 3009 3412 2942 \nQ 3269 2875 3167 2744 \nQ 3066 2613 3011 2416 \nQ 2956 2219 2956 1959 \nL 2956 0 \nL 2400 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-73\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"133.300781\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"211.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <path d=\"M 42.003125 285.8475 \nL 399.123125 285.8475 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_24\"/>\n     <g id=\"text_12\">\n      <!-- &lt;unk&gt; -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 289.470937) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-3c\" d=\"M 316 1784 \nL 316 2425 \nL 3425 3731 \nL 3425 3250 \nL 744 2106 \nL 3425 959 \nL 3425 481 \nL 316 1784 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-75\" d=\"M 981 3381 \nL 981 1238 \nQ 981 991 1017 825 \nQ 1053 659 1134 557 \nQ 1216 456 1348 414 \nQ 1481 372 1678 372 \nQ 1878 372 2039 442 \nQ 2200 513 2314 645 \nQ 2428 778 2490 973 \nQ 2553 1169 2553 1422 \nL 2553 3381 \nL 3116 3381 \nL 3116 722 \nQ 3116 616 3117 498 \nQ 3119 381 3122 276 \nQ 3125 172 3128 97 \nQ 3131 22 3134 0 \nL 2603 0 \nQ 2600 16 2597 84 \nQ 2594 153 2589 242 \nQ 2584 331 2581 423 \nQ 2578 516 2578 578 \nL 2569 578 \nQ 2488 431 2391 312 \nQ 2294 194 2166 111 \nQ 2038 28 1872 -17 \nQ 1706 -63 1488 -63 \nQ 1206 -63 1003 6 \nQ 800 75 669 219 \nQ 538 363 477 588 \nQ 416 813 416 1128 \nL 416 3381 \nL 981 3381 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-3e\" d=\"M 316 481 \nL 316 959 \nL 2997 2106 \nL 316 3250 \nL 316 3731 \nL 3425 2425 \nL 3425 1784 \nL 316 481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-3c\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"58.398438\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"114.013672\"/>\n       <use xlink:href=\"#LiberationSans-6b\" x=\"169.628906\"/>\n       <use xlink:href=\"#LiberationSans-3e\" x=\"219.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_25\">\n      <path d=\"M 42.003125 323.9625 \nL 399.123125 323.9625 \n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_26\"/>\n     <g id=\"text_13\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(32.225 327.585937) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 42.003125 38.1 \nL 42.003125 76.215 \nL 113.427125 76.215 \nL 113.427125 38.1 \nL 42.003125 38.1 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084e98\"/>\n    <path d=\"M 113.427125 38.1 \nL 113.427125 76.215 \nL 184.851125 76.215 \nL 184.851125 38.1 \nL 113.427125 38.1 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #7cb7da\"/>\n    <path d=\"M 184.851125 38.1 \nL 184.851125 76.215 \nL 256.275125 76.215 \nL 256.275125 38.1 \nL 184.851125 38.1 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #0e59a2\"/>\n    <path d=\"M 256.275125 38.1 \nL 256.275125 76.215 \nL 327.699125 76.215 \nL 327.699125 38.1 \nL 256.275125 38.1 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #1a68ae\"/>\n    <path d=\"M 327.699125 38.1 \nL 327.699125 76.215 \nL 399.123125 76.215 \nL 399.123125 38.1 \nL 327.699125 38.1 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083573\"/>\n    <path d=\"M 42.003125 76.215 \nL 42.003125 114.33 \nL 113.427125 114.33 \nL 113.427125 76.215 \nL 42.003125 76.215 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083a7a\"/>\n    <path d=\"M 113.427125 76.215 \nL 113.427125 114.33 \nL 184.851125 114.33 \nL 184.851125 76.215 \nL 113.427125 76.215 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #ddeaf7\"/>\n    <path d=\"M 184.851125 76.215 \nL 184.851125 114.33 \nL 256.275125 114.33 \nL 256.275125 76.215 \nL 184.851125 76.215 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084488\"/>\n    <path d=\"M 256.275125 76.215 \nL 256.275125 114.33 \nL 327.699125 114.33 \nL 327.699125 76.215 \nL 256.275125 76.215 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083a7a\"/>\n    <path d=\"M 327.699125 76.215 \nL 327.699125 114.33 \nL 399.123125 114.33 \nL 399.123125 76.215 \nL 327.699125 76.215 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083674\"/>\n    <path d=\"M 42.003125 114.33 \nL 42.003125 152.445 \nL 113.427125 152.445 \nL 113.427125 114.33 \nL 42.003125 114.33 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083979\"/>\n    <path d=\"M 113.427125 114.33 \nL 113.427125 152.445 \nL 184.851125 152.445 \nL 184.851125 114.33 \nL 113.427125 114.33 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #3686c0\"/>\n    <path d=\"M 184.851125 114.33 \nL 184.851125 152.445 \nL 256.275125 152.445 \nL 256.275125 114.33 \nL 184.851125 114.33 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #69add5\"/>\n    <path d=\"M 256.275125 114.33 \nL 256.275125 152.445 \nL 327.699125 152.445 \nL 327.699125 114.33 \nL 256.275125 114.33 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #09529d\"/>\n    <path d=\"M 327.699125 114.33 \nL 327.699125 152.445 \nL 399.123125 152.445 \nL 399.123125 114.33 \nL 327.699125 114.33 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084082\"/>\n    <path d=\"M 42.003125 152.445 \nL 42.003125 190.56 \nL 113.427125 190.56 \nL 113.427125 152.445 \nL 42.003125 152.445 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08316d\"/>\n    <path d=\"M 113.427125 152.445 \nL 113.427125 190.56 \nL 184.851125 190.56 \nL 184.851125 152.445 \nL 113.427125 152.445 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083877\"/>\n    <path d=\"M 184.851125 152.445 \nL 184.851125 190.56 \nL 256.275125 190.56 \nL 256.275125 152.445 \nL 184.851125 152.445 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084c95\"/>\n    <path d=\"M 256.275125 152.445 \nL 256.275125 190.56 \nL 327.699125 190.56 \nL 327.699125 152.445 \nL 256.275125 152.445 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #71b1d7\"/>\n    <path d=\"M 327.699125 152.445 \nL 327.699125 190.56 \nL 399.123125 190.56 \nL 399.123125 152.445 \nL 327.699125 152.445 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #4896c8\"/>\n    <path d=\"M 42.003125 190.56 \nL 42.003125 228.675 \nL 113.427125 228.675 \nL 113.427125 190.56 \nL 42.003125 190.56 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08306b\"/>\n    <path d=\"M 113.427125 190.56 \nL 113.427125 228.675 \nL 184.851125 228.675 \nL 184.851125 190.56 \nL 113.427125 190.56 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083573\"/>\n    <path d=\"M 184.851125 190.56 \nL 184.851125 228.675 \nL 256.275125 228.675 \nL 256.275125 190.56 \nL 184.851125 190.56 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08326e\"/>\n    <path d=\"M 256.275125 190.56 \nL 256.275125 228.675 \nL 327.699125 228.675 \nL 327.699125 190.56 \nL 256.275125 190.56 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083d7f\"/>\n    <path d=\"M 327.699125 190.56 \nL 327.699125 228.675 \nL 399.123125 228.675 \nL 399.123125 190.56 \nL 327.699125 190.56 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #f2f7fd\"/>\n    <path d=\"M 42.003125 228.675 \nL 42.003125 266.79 \nL 113.427125 266.79 \nL 113.427125 228.675 \nL 42.003125 228.675 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08306b\"/>\n    <path d=\"M 113.427125 228.675 \nL 113.427125 266.79 \nL 184.851125 266.79 \nL 184.851125 228.675 \nL 113.427125 228.675 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #083979\"/>\n    <path d=\"M 184.851125 228.675 \nL 184.851125 266.79 \nL 256.275125 266.79 \nL 256.275125 228.675 \nL 184.851125 228.675 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08316d\"/>\n    <path d=\"M 256.275125 228.675 \nL 256.275125 266.79 \nL 327.699125 266.79 \nL 327.699125 228.675 \nL 256.275125 228.675 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08326e\"/>\n    <path d=\"M 327.699125 228.675 \nL 327.699125 266.79 \nL 399.123125 266.79 \nL 399.123125 228.675 \nL 327.699125 228.675 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #f7fbff\"/>\n    <path d=\"M 42.003125 266.79 \nL 42.003125 304.905 \nL 113.427125 304.905 \nL 113.427125 266.79 \nL 42.003125 266.79 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08326e\"/>\n    <path d=\"M 113.427125 266.79 \nL 113.427125 304.905 \nL 184.851125 304.905 \nL 184.851125 266.79 \nL 113.427125 266.79 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #08488e\"/>\n    <path d=\"M 184.851125 266.79 \nL 184.851125 304.905 \nL 256.275125 304.905 \nL 256.275125 266.79 \nL 184.851125 266.79 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #0c56a0\"/>\n    <path d=\"M 256.275125 266.79 \nL 256.275125 304.905 \nL 327.699125 304.905 \nL 327.699125 266.79 \nL 256.275125 266.79 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084488\"/>\n    <path d=\"M 327.699125 266.79 \nL 327.699125 304.905 \nL 399.123125 304.905 \nL 399.123125 266.79 \nL 327.699125 266.79 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #c1d9ed\"/>\n    <path d=\"M 42.003125 304.905 \nL 42.003125 343.02 \nL 113.427125 343.02 \nL 113.427125 304.905 \nL 42.003125 304.905 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #084285\"/>\n    <path d=\"M 113.427125 304.905 \nL 113.427125 343.02 \nL 184.851125 343.02 \nL 184.851125 304.905 \nL 113.427125 304.905 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #1865ac\"/>\n    <path d=\"M 184.851125 304.905 \nL 184.851125 343.02 \nL 256.275125 343.02 \nL 256.275125 304.905 \nL 184.851125 304.905 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #1562a9\"/>\n    <path d=\"M 256.275125 304.905 \nL 256.275125 343.02 \nL 327.699125 343.02 \nL 327.699125 304.905 \nL 256.275125 304.905 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #1562a9\"/>\n    <path d=\"M 327.699125 304.905 \nL 327.699125 343.02 \nL 399.123125 343.02 \nL 399.123125 304.905 \nL 327.699125 304.905 \nz\n\" clip-path=\"url(#p081ace14a1)\" style=\"fill: #4896c8\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.003125 343.02 \nL 42.003125 38.1 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 399.123125 343.02 \nL 399.123125 38.1 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.003125 343.02 \nL 399.123125 343.02 \n\" style=\"fill: none\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.003125 38.1 \nL 399.123125 38.1 \n\" style=\"fill: none\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 421.443125 343.02 \nL 436.689125 343.02 \nL 436.689125 38.1 \nL 421.443125 38.1 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_27\"/>\n     <g id=\"text_14\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 282.435191) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_28\"/>\n     <g id=\"text_15\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 218.092634) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_29\"/>\n     <g id=\"text_16\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 153.750077) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_30\"/>\n     <g id=\"text_17\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(443.689125 89.40752) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABYAAAGnCAYAAACzVadfAAACF0lEQVR4nO2cu63EMBADJUPBK+f6uv7je4Etyg0wGGCmAGLBpVYfA55/n+9vFLgaomOMscbsaK8xZ0WYZ0Wz4o7HWhF4cdOKwIubVgRe3IjNu8zxjTl+C2vFDS9uWhF4cdOKwKvY5gXerNCK0Kt4tprXEm42ryNs8wLPCl7F5rgvzLOCV7E5DryKgVYYt7owzwpexeY48CoGWmHc6sI8K3gVm+PAq3iVdLXiwLOiWXFJ+Cp9++dZwYubVgRexTYv8GaFVgRe3LQi8OKmFYEXN60IvIptXuDNCq04wpdWPPBW3io9VwCt4MVtzdKTd88K4/YSruhWm0eLm9v/xrgd4ZITwCHEG/RAK3grz9Pmhhi3lrDb/0tYK26I2z/OCl7cWspEK3Bx8+y2ATbPZ95NbQjZvMCzgjcrbF4ANs83+g2vYg8sR5i4pG3eIwxsHm5sFs9uHWFXXvDgHZCzoiTM2/55nzb9yrsBrrzeh8KKLNEK3qwAWmHcjnBF1rH5ona5AVrReh4DWuGVty7MyzHxsQlnhXGLMO+xybgd4Y4u8PYPvDUBf1/lyotwR9e//h28KgTjdoQdQke4owtMhUMoOISCQygYtyPsENrw4uYQCry4OYQCL24OocCLm0Mo8OIGHEI1K4apeAAOIaIVtLgVt/8OrrzQO3gTt/8OvLgBhxBu5WlFMG4v4Y4ucPvnxQ258jrw4uYQCv/ARQk69n7x0gAAAABJRU5ErkJggg==\" id=\"image6493a2e476\" transform=\"scale(1 -1) translate(0 -304.56)\" x=\"421.2\" y=\"-38.16\" width=\"15.84\" height=\"304.56\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 421.443125 343.02 \nL 429.066125 343.02 \nL 436.689125 343.02 \nL 436.689125 38.1 \nL 429.066125 38.1 \nL 421.443125 38.1 \nL 421.443125 343.02 \nz\n\" style=\"fill: none\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p081ace14a1\">\n   <rect x=\"42.003125\" y=\"38.1\" width=\"357.12\" height=\"304.92\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report:\n",
        "\n",
        "We tried the sentences \"das ist ein haus\", \"mein vater ist klein\" and \"die katze ist nicht klein\". These should have been translated to \"That is a house\", \"My father is small\" and \"The cat is not small\". The main pattern we look out for is the white diagonal line, from top left to the bottom right. However since the word order is not necessarily the same, it might differ a bit. For our sentences the model performed decently, with white spots corresponding more or less to the correct words, meaning the model pays attention to the correct things. Sometimes there is some abiguity with random words, for example the last sentece mapping \"klein\" fairly confidently to both \"small\" and \"a\".\n",
        "\n",
        "Based on what we know about attention the results were more or less expected, the model mostly maps correct words to each other. Especially for the main nouns and adjectives, klein -> small, haus -> house etc. For the common grammar words it's usually a bit less confident. This might be because they occur a lot in different places so it might be hard to create a consistent mapping.\n",
        "\n",
        "We learned how to build a model that's more sophisticated than the legacy ones, since we use the concept of attention which takes more context into account when assigning probability to the word mappings. We also learned how to resize tensors, and that Colab is sometimes a bit tricky to work with.\n",
        "\n"
      ],
      "metadata": {
        "id": "k3DMN-RW-aNF"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "12f500e95db8c7000aae6810a3b3f88d1298056c5758c6b1fc4b18ff2c238050"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}