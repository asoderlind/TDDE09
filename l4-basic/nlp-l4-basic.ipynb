{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4: Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement a simplified version of the dependency parser presented by [Glavaš and Vulić (2021)](http://dx.doi.org/10.18653/v1/2021.eacl-main.270). This parser consists of a transformer encoder followed by a bi-affine layer that computes arc scores for all pairs of words. These scores are then used as logits in a classifier that predicts the position of the head of each word. In contrast to the parser described in the paper, yours will only support unlabelled parsing, that is, you will not implement what the authors call a *relation classifier*. As the encoder, you will use the [uncased DistilBERT base model](https://huggingface.co/docs/transformers/model_doc/distilbert) from the [Transformers](https://huggingface.co/docs/transformers/main/en/index) library, even though every other BERT-based encoder should work equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab comes from the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). To read the data, we use the [CoNLL-U Parser](https://pypi.org/project/conllu/) library. The code in the next cell defines a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) wrapper for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ParserDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.items = []\n",
    "        with open(filename, 'rt', encoding='utf-8') as fp:\n",
    "            for tokens in conllu.parse_incr(fp):\n",
    "                self.items.append([(t['form'], t['head']) for t in tokens if isinstance(t['id'], int)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = ParserDataset('en_ewt-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of **parsed sentences**. A parsed sentence is represented as a list of pairs. The first component of each pair (a string) represents a word; the second component (an integer) specifies the position of the word’s head, i.e., its parent in the dependency tree. Note that word positions are numbered starting at&nbsp;1; the head position&nbsp;0 represents the root of the tree.\n",
    "\n",
    "Run the next cell to see an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('I', 'like', 'yuor', 'blog', '.'), (2, 0, 4, 2, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_WORDS: tuple[str] = list(zip(*TRAIN_DATA[531]))[0]\n",
    "EXAMPLE_HEADS: tuple[int] = list(zip(*TRAIN_DATA[531]))[1]\n",
    "\n",
    "EXAMPLE_WORDS, EXAMPLE_HEADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example sentence consists of five whitespace-separated words, including the final punctuation mark. The head of the pronoun *I* is the word at position&nbsp;2 – the verb *like*. The head of the word *like* is the root of the tree (position&nbsp;0). The dependents of *like* are *I* (position&nbsp;1), the noun *blog* (position&nbsp;4), and the final punctuation mark. Note that the pronoun *your* (position&nbsp;3) is misspelled as *yuor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Tokenisation (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed parsed sentences to DistilBERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the DistilBERT tokeniser using the [Auto classes](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "TOKENIZER: PreTrainedTokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the tokeniser on the example sentence as shown in the next cell. Note that we use the *is_split_into_words* keyword argument to indicate that the input is already pre-tokenised (split on whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TOKENS = TOKENIZER(EXAMPLE_WORDS, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the tokeniser is an object of class [`BatchEncoding`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding). The code in the following cell shows the list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'like', 'yu', '##or', 'blog', '.', '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TOKENIZER.decode(i) for i in EXAMPLE_TOKENS.input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokeniser adds the special tokens `[CLS]` and `[SEP]` and splits unknown words (here: the misspelled word *yuor*) into sub-word tokens. We will need to keep track of which tokens correspond to which word. To achieve that, we can use the method [`word_to_tokens()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_to_tokens), which gets us the encoded token span (an object of class [`TokenSpan`](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.TokenSpan)) corresponding to a word in a sequence of the input batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenSpan(start=1, end=2) -> I\n",
      "TokenSpan(start=2, end=3) -> like\n",
      "TokenSpan(start=3, end=5) -> yuor\n",
      "TokenSpan(start=5, end=6) -> blog\n",
      "TokenSpan(start=6, end=7) -> .\n"
     ]
    }
   ],
   "source": [
    "from transformers import TokenSpan\n",
    "\n",
    "for i, word in enumerate(EXAMPLE_WORDS):\n",
    "    token_span: TokenSpan = EXAMPLE_TOKENS.word_to_tokens(i)\n",
    "    print(token_span, '->', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to code a function `encode()` that takes a tokeniser and a list of sentences and returns the tokeniser&rsquo;s encoded input as well as the corresponding token spans. The following cell contains skeleton code for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "from typing import Iterable\n",
    "import torch\n",
    "\n",
    "def encode(tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, sentences: list[list[tuple[str, int]]]) -> tuple[BatchEncoding, list[list[TokenSpan]]]:\n",
    "    '''\n",
    "    Returns tokeniser's encoded input and corresponding token spans\n",
    "    '''\n",
    "    \n",
    "    split_sentence_list = [[word for word, _ in sentence] for sentence in sentences]\n",
    "\n",
    "    tokens: BatchEncoding = tokenizer(split_sentence_list, truncation=True, padding=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens.input_ids = torch.tensor(tokens.input_ids)\n",
    "\n",
    "    token_spans_list: list[list[TokenSpan]] = []\n",
    "    \n",
    "    for i in range(len(split_sentence_list)):\n",
    "        token_spans = []\n",
    "        for j in range(len(split_sentence_list[i])):\n",
    "            token_spans.append(tokens.word_to_tokens(i, j))\n",
    "        token_spans_list.append(token_spans)\n",
    "\n",
    "    return tokens, token_spans_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this function to match the following specification:\n",
    "\n",
    "**encode** (*tokenizer*, *sentences*):\n",
    "\n",
    "> Uses the specified *tokenizer* to encode a batch of parsed sentences (*sentences*). Returns a pair consisting of a [`BatchEncoding`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding) and a matching batch of token spans (as explained above). The [`BatchEncoding`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding) is the standard batch encoding, but with tensors instead of lists of Python integers. Sentences have been truncated to the maximum acceptable input length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, call `encode()` on a small number of sentences and check that output matches your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2066, 9805, 2953, 9927, 1012,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 3531, 4638, 2041, 2026, 2040, 2292, 1996, 3899, 2041, 9927, 1012,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4931, 2008, 1005, 1055, 1037, 2307, 9927, 1010, 1045, 2036, 2031,\n",
      "         1037, 2609, 2055, 3050, 3349, 3784, 5306,  102]])\n",
      "[TokenSpan(start=1, end=2), TokenSpan(start=2, end=3), TokenSpan(start=3, end=5), TokenSpan(start=5, end=6), TokenSpan(start=6, end=7)]\n",
      "[TokenSpan(start=1, end=2), TokenSpan(start=2, end=3), TokenSpan(start=3, end=4), TokenSpan(start=4, end=5), TokenSpan(start=5, end=6), TokenSpan(start=6, end=7), TokenSpan(start=7, end=8), TokenSpan(start=8, end=9), TokenSpan(start=9, end=10), TokenSpan(start=10, end=11), TokenSpan(start=11, end=12)]\n",
      "[TokenSpan(start=1, end=2), TokenSpan(start=2, end=3), TokenSpan(start=3, end=5), TokenSpan(start=5, end=6), TokenSpan(start=6, end=7), TokenSpan(start=7, end=8), TokenSpan(start=8, end=9), TokenSpan(start=9, end=10), TokenSpan(start=10, end=11), TokenSpan(start=11, end=12), TokenSpan(start=12, end=13), TokenSpan(start=13, end=14), TokenSpan(start=14, end=15), TokenSpan(start=15, end=16), TokenSpan(start=16, end=17), TokenSpan(start=17, end=18), TokenSpan(start=18, end=19)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/15p7683j6vz_5pj6pk23sx1c0000gn/T/ipykernel_97008/1440010210.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens.input_ids = torch.tensor(tokens.input_ids)\n"
     ]
    }
   ],
   "source": [
    "# Test the encode() function\n",
    "sentences = TRAIN_DATA[531:534]\n",
    "encoded_batch = encode(TOKENIZER, sentences)\n",
    "\n",
    "print(encoded_batch[0]['input_ids'])\n",
    "\n",
    "for token_spans in encoded_batch[1]:\n",
    "    print(token_spans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Merging tokens (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT gives us a representation for each *token* in a sentence. To compute scores between pairs of *words*, we will need to combine the token representations that correspond to each word. A standard strategy for this is to take their element-wise mean. The next cell contains skeleton code for a function `merge_tokens()` that implements this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def merge_tokens(tokens: Tensor, token_spans_list: list[list[TokenSpan]]):\n",
    "    '''\n",
    "    tokens: Tensor[batch_size, num_tokens, hidden_dim]\n",
    "    token_spans: list[TokenSpan]\n",
    "    return word_level_representations: Tensor[batch_size, max_num_words, hidden_dim]\n",
    "    '''\n",
    "    batch_size, num_tokens, hidden_dim = tokens.shape\n",
    "    max_num_words = max(len(token_spans) for token_spans in token_spans_list)\n",
    "    word_level_representations = tokens.new_zeros(batch_size, max_num_words, hidden_dim)\n",
    "\n",
    "    for i, token_spans in enumerate(token_spans_list):  # Iterate through each sentence in the batch\n",
    "        for j, (start, end) in enumerate(token_spans):  # Iterate through each word's span in the sentence\n",
    "            # Calculate the mean of the tokens for this word. Note that `end` is exclusive, so we add 1 to include it.\n",
    "            word_tensor = tokens[i, start:end].mean(dim=0)  \n",
    "            # Assign the mean tensor to the correct position in the word-level representations tensor.\n",
    "            word_level_representations[i, j] = word_tensor\n",
    "\n",
    "\n",
    "    return word_level_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this function to match the following specification:\n",
    "\n",
    "**merge_tokens** (*tokens*, *token_spans*)\n",
    "\n",
    "> Takes a batch of token vectors (*tokens*) and a list of matching token spans (*token_spans*) and returns a batch of word-level representations, computed using the element-wise mean. The token vectors are a tensor of shape (*batch_size*, *num_tokens*, *hidden_dim*), where *hidden_dim* is the dimensionality of the DistilBERT representations. The token spans are a nested list containing integer pairs, as computed in Problem&nbsp;1. The result is a tensor of shape (*batch_size*, *max_num_words*, *hidden_dim*), where *max_num_words* denotes the maximum number of words in any sentence in the batch. Entries corresponding to padding are represented by the zero vector of size *hidden_dim*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, create a sample input to `merge_tokens()` and check that the output matches your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2097, 0.5334, 0.6464],\n",
      "         [0.4675, 0.5390, 0.2383],\n",
      "         [0.5887, 0.4826, 0.6153],\n",
      "         [0.2409, 0.5102, 0.4143],\n",
      "         [0.7184, 0.7848, 0.2965]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/15p7683j6vz_5pj6pk23sx1c0000gn/T/ipykernel_97008/1440010210.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens.input_ids = torch.tensor(tokens.input_ids)\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor, rand\n",
    "\n",
    "tokens = rand(1, 1000, 3)\n",
    "\n",
    "# Test the encode() function\n",
    "sentences = TRAIN_DATA[531:532]\n",
    "_, token_spans_list = encode(TOKENIZER, sentences)\n",
    "\n",
    "out = merge_tokens(tokens, token_spans_list)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Biaffine layer (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the bi-affine layer. Given matrices $X \\in \\mathbb{R}^{m \\times d}$ and $X' \\in \\mathbb{R}^{n \\times d}$, this layer computes a matrix $Y \\in \\mathbb{R}^{m \\times n}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y = X W X'{}^\\top + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $W \\in \\mathbb{R}^{d \\times d}$ and $b \\in \\mathbb{R}$ are learnable weight and bias parameters. In the context of the dependency parser, the matrices $X$ and $X'$ hold the word representations of all dependents and all heads in the input sentence, and the entries of the matrix $Y$ are interpreted as scores of possible dependency arcs. More specifically, the entry $Y_{ij}$ represents the score of an arc from a head word at position&nbsp;$j$ to a dependent at position&nbsp;$i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following cell contains skeleton code for the implementation of the bi-affine layer. Implement this layer according to the specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Biaffine(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim):\n",
    "        super().__init__()\n",
    "        # Weight matrix for the bi-affine transformation, shape: (d, d)\n",
    "        self.weight = nn.Parameter(torch.rand(encoder_dim, encoder_dim))\n",
    "        # Bias term\n",
    "        self.bias = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        # x1 shape: (batch_size, m, d), x2 shape: (batch_size, n, d)\n",
    "        # We perform a batch matrix multiplication between x1 and the weight, resulting in a shape of (batch_size, m, d)\n",
    "        y = x1 @ self.weight @ x2.transpose(1, 2) + self.bias\n",
    "\n",
    "        \"\"\" # Then we perform another batch matrix multiplication, this time with the transpose of x2.\n",
    "        # For this, we need to transpose x2 to shape (batch_size, d, n) to align the dimensions for matrix multiplication.\n",
    "        # The resulting shape of the operation will be (batch_size, m, n), aligning with our target output shape.\n",
    "        y = torch.bmm(x1_transform, x2.transpose(1, 2))\n",
    "\n",
    "        # Adding the bias term to every element in the output matrix.\n",
    "        # Since the bias is a scalar, it's automatically broadcasted across all dimensions.\n",
    "        y += self.bias \"\"\"\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️ Note that your implementation should be able to handle *batches* of input sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, create a sample input to the bi-affine layer as well as suitable weights and biases and check that the output of the `forward` method matches your expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "word_level_representations = rand(1, 12, 3)\n",
    "word_level_representations2 = rand(1, 13, 3)\n",
    "biaffine = Biaffine(3)\n",
    "out = biaffine(word_level_representations, word_level_representations2)\n",
    "print(out.shape) # should be (1, 12, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Parser (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to put the two main components of the parser together: the encoder (DistilBert) and the bi-affine layer that computes the arc scores. We also add a dropout layer between the two components.\n",
    "\n",
    "The following code cell contains skeleton code for the parsing model with the `init()` method already complete. Your task is to implement the `forward()` method. If you are unsure how things should be wired up, have another look at the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertPreTrainedModel\n",
    "\n",
    "class DistilBertForParsing(DistilBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, dropout=0.1):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.biaffine = Biaffine(config.hidden_size)\n",
    "\n",
    "    def forward(self, encoded_input: BatchEncoding, token_spans: list[list[TokenSpan]]) -> torch.Tensor:\n",
    "        '''\n",
    "        returns: torch.Tensor[batch_size, num_words, num_words + 1]\n",
    "        '''\n",
    "        # Encode the input\n",
    "        bert_outputs = self.distilbert(**encoded_input)\n",
    "        # Merge the tokens into word-level representations\n",
    "        word_representations = self.dropout(merge_tokens(bert_outputs.last_hidden_state, token_spans))\n",
    "        # Apply the bi-affine transformation\n",
    "        scores = self.biaffine(word_representations, word_representations)\n",
    "        batch_size, num_words, _ = scores.shape\n",
    "        root_scores = torch.cat([torch.zeros(batch_size, num_words, 1, device=scores.device), scores], dim=2)\n",
    "        \n",
    "        return root_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `forward()` method to match the following specification:\n",
    "\n",
    "**forward** (*encoded_input*, *token_spans*)\n",
    "\n",
    "> Takes a tokeniser-encoded batch of sentences (*encoded_input*, of type `BatchEncoding`) and a corresponding nested list of token spans (*token_spans*) and returns a tensor with scores for each pair of words. More specifically, the output tensor $Y$ has shape (*batch_size*, *num_words*, *num_words+1*), where the entry $Y_{bij}$ represents the score of an arc from a head word at position&nbsp;$j$ to a dependent at position&nbsp;$i$ in the $b$th sentence of the batch. Note that the number of possible heads is one greater than the number of possible dependents because the possible heads include the root of the dependency tree, which we represent using the special token `[CLS]` (at position&nbsp;0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, instantiate the parsing model and feed it a small batch of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForParsing were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['biaffine.bias', 'biaffine.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 2), ('like', 0), ('yuor', 4), ('blog', 2), ('.', 2)]\n",
      "[('Please', 2), ('check', 0), ('out', 2), ('my', 10), ('who', 6), ('let', 10), ('the', 8), ('dog', 6), ('out', 6), ('blog', 2), ('.', 2)]\n",
      "[('hey', 6), ('that', 6), (\"'s\", 6), ('a', 6), ('great', 6), ('blog', 0), (',', 10), ('I', 10), ('also', 10), ('have', 6), ('a', 12), ('site', 10), ('about', 17), ('los', 15), ('angeles', 17), ('online', 17), ('dating', 12)]\n",
      "tensor([[[ 0.0000e+00, -2.6719e+00,  2.7973e+01,  2.8288e+01,  1.0362e+01,\n",
      "          -1.8299e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -3.4884e+01, -4.1674e+01, -5.0170e+00, -8.9987e+00,\n",
      "           2.9503e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -9.9223e+00, -1.6085e+00,  6.3764e+00, -1.2575e+01,\n",
      "           3.4243e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -3.3685e+01, -4.8049e+01, -1.5399e+01, -1.5100e+01,\n",
      "           3.5496e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -5.0243e+01, -1.1589e+01,  6.3141e+00, -1.8092e+01,\n",
      "          -3.1233e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  2.2377e+01,  2.2055e+01,  5.4136e+01,  6.2021e+01,\n",
      "           1.1706e+01, -5.7708e+00,  3.2950e+01,  4.4304e+01,  5.9045e+01,\n",
      "           2.7495e+01,  3.2600e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -1.1475e+01, -3.5926e+00, -1.1084e+01,  2.4930e+01,\n",
      "          -1.3303e+00,  6.7336e-01,  4.4650e+01,  9.1066e+00,  1.6209e+00,\n",
      "          -1.4363e+00,  1.9124e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.7138e+01,  1.6187e+01,  3.4158e+01,  5.3072e+01,\n",
      "           1.1828e+01,  1.8983e+01,  5.7316e+01,  1.3895e+01,  3.4110e+01,\n",
      "           2.1986e+01,  2.3926e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8155e+00, -5.2587e+00,  5.2409e+00,  1.4102e+01,\n",
      "           3.3031e+00, -1.4697e+01,  2.9115e+01,  2.2703e+01,  3.0378e+01,\n",
      "           1.0952e+01,  3.8355e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -2.8943e-02, -2.7677e+00, -3.8985e+00, -1.6595e+01,\n",
      "          -3.7642e+00, -2.1733e+01,  2.5175e+01,  1.0546e+01,  4.3495e+00,\n",
      "          -1.5395e+01,  4.9891e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  2.1697e+01,  1.4604e+00,  8.5183e+00,  1.8414e+01,\n",
      "          -6.6155e+00,  8.1090e+00,  5.4607e+01,  2.3039e+01,  1.7383e+01,\n",
      "          -1.9253e+01,  2.9089e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  5.4135e+01,  8.9335e+00,  2.3731e+01,  3.2956e+01,\n",
      "           2.6287e+01,  3.1567e+01,  7.0609e+01,  3.2084e+01,  4.3563e+01,\n",
      "           1.6163e+00,  3.0831e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  6.1826e+00,  5.1696e+00,  1.8451e+00, -1.4099e+00,\n",
      "          -1.3397e+01,  1.2984e+00,  2.3528e+01,  1.7961e+01,  1.4579e+01,\n",
      "          -6.0763e+00,  1.7680e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  6.6808e+01,  4.7302e+01,  5.1100e+01,  4.4148e+01,\n",
      "           1.6825e+01,  2.0159e+01,  5.1086e+01,  1.2910e+01,  4.9940e+01,\n",
      "           2.3631e+01,  3.0354e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -1.5202e+01, -2.8207e+01, -2.4934e+01, -2.8951e+01,\n",
      "          -4.4916e+01, -4.9832e+01, -1.6363e+01, -6.2251e+00, -6.8328e+00,\n",
      "          -1.6102e+01,  4.9760e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00, -2.0177e+01, -2.0532e+01, -1.8384e+01,  2.9833e+01,\n",
      "           3.3635e+01,  2.2244e+01,  1.4512e+01, -5.6991e+00,  1.1439e+01,\n",
      "          -8.1464e+00, -1.5978e+01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01],\n",
      "         [ 0.0000e+00,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,  3.8844e-01,\n",
      "           3.8844e-01,  3.8844e-01,  3.8844e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  5.5012e+01,  3.1313e+01,  4.0057e+01,  6.1567e+01,\n",
      "           5.4975e+01,  3.1558e+01,  6.9931e+01,  1.8342e+01,  2.9335e+01,\n",
      "           2.6058e+01,  5.6844e+01,  4.7790e+01,  3.7768e+01,  2.3458e+01,\n",
      "           1.9269e+01,  2.9534e+01,  4.8038e+01],\n",
      "         [ 0.0000e+00,  1.4908e+01, -6.3568e+00, -5.3751e+00,  2.8760e+00,\n",
      "          -1.6633e+01, -3.8034e+01,  1.2263e+01, -1.3081e+01, -3.3378e+01,\n",
      "          -4.7012e+01, -1.7589e+01, -3.1967e+01, -3.7979e+01, -5.6403e+01,\n",
      "          -3.9384e+01, -7.9889e+00,  7.4945e+00],\n",
      "         [ 0.0000e+00,  3.6728e+01, -7.1131e+00,  5.7530e+00, -8.8108e+00,\n",
      "          -2.1102e+00, -8.9712e+00,  6.9665e+01, -8.9515e+00, -8.1264e+00,\n",
      "          -1.0453e+01, -3.3295e+01, -1.7636e+01, -1.4346e+01, -1.2470e+01,\n",
      "           4.6449e+00,  3.6254e+01,  4.5121e+01],\n",
      "         [ 0.0000e+00,  3.2909e+01, -1.2757e+01, -1.0807e+01, -1.4832e+01,\n",
      "          -2.8746e+01, -2.5412e+01,  5.3876e+01,  1.8292e-01, -4.8996e+01,\n",
      "          -4.8534e+01, -2.1557e+01, -1.9678e+01, -4.5647e+00, -4.1814e+01,\n",
      "          -8.8415e+00,  1.2678e+00,  6.6813e+00],\n",
      "         [ 0.0000e+00,  4.3500e+00, -2.6055e+01,  2.4077e+01, -2.4576e+01,\n",
      "          -2.7329e+01, -2.4811e+01,  4.0841e+01, -3.0137e+01, -4.8868e+01,\n",
      "          -5.4654e+01, -3.4452e+01, -1.5530e+01,  6.9209e+00, -1.5460e+01,\n",
      "          -1.1735e+00,  1.0654e+00,  1.9065e+01],\n",
      "         [ 0.0000e+00, -1.5400e+01, -6.7800e+01, -1.0597e+01, -3.7908e+01,\n",
      "          -5.5187e+01, -1.7528e+01, -1.4063e+01, -5.6141e+01, -6.7325e+01,\n",
      "          -6.0003e+01, -4.1033e+01, -2.7953e+01, -6.2207e+01, -6.0549e+01,\n",
      "          -3.9258e+01, -2.1684e+01, -1.1366e+01],\n",
      "         [ 0.0000e+00,  3.2251e+01,  3.4408e+01,  3.4959e+00,  2.5520e+01,\n",
      "           4.4473e+01, -1.9992e+01,  6.8492e+01, -8.9708e+00,  2.6247e+01,\n",
      "           1.7158e+01,  9.1209e+00, -1.4611e+01, -3.1800e+01,  1.3430e+01,\n",
      "           1.7838e+00, -9.8390e-01,  2.2565e+01],\n",
      "         [ 0.0000e+00,  2.3390e+01, -5.6613e+00,  1.7957e+01,  2.0394e+01,\n",
      "           2.0199e+01, -8.0737e+00,  6.3732e+01, -3.4205e+00,  9.5579e+00,\n",
      "           1.4468e+01,  2.2452e+01,  1.2442e+00,  7.0910e+00, -2.3967e+01,\n",
      "          -2.1891e+00,  1.1343e+01,  2.8457e+01],\n",
      "         [ 0.0000e+00,  3.7879e+01, -1.8975e+01,  3.6718e+01,  9.6643e+00,\n",
      "          -1.0851e+00, -1.8733e+01,  4.5374e+01, -4.1402e-01,  1.0974e+01,\n",
      "          -1.7741e+00,  5.7576e+00, -2.3062e+01,  1.1200e+01, -3.0559e+01,\n",
      "          -1.5341e+01,  6.4257e+00,  1.6582e+01],\n",
      "         [ 0.0000e+00,  3.2381e+01, -4.1837e+00,  4.6423e+01,  2.0304e+01,\n",
      "           5.6590e-01, -1.3324e+01,  5.2353e+01, -1.2108e+01,  2.8036e+00,\n",
      "           1.7795e-01,  1.3673e+01, -9.2566e+00,  8.3334e+00, -2.4510e+01,\n",
      "          -1.7488e+01,  2.0805e+01,  3.3732e+01],\n",
      "         [ 0.0000e+00,  3.9573e+01, -8.5299e+00,  2.9579e+01,  6.0737e+00,\n",
      "          -4.3576e+00, -1.6830e+00,  4.9368e+01, -4.4348e+00, -1.7461e+01,\n",
      "          -3.8946e+01, -5.0225e+00, -5.5756e-01,  1.6598e+01, -1.8160e+01,\n",
      "           1.3719e+01,  2.7022e+01,  4.7747e+01],\n",
      "         [ 0.0000e+00,  1.7368e+01, -5.0162e+01,  6.7053e+00, -2.4953e+01,\n",
      "          -3.2040e+01,  1.2067e+01, -4.0038e+00, -4.1226e+01, -4.2932e+01,\n",
      "          -5.4059e+01, -3.0094e+01,  4.8018e+00, -1.3950e+00, -3.5237e+01,\n",
      "          -3.6172e+00,  1.5788e+01,  2.0809e+01],\n",
      "         [ 0.0000e+00,  1.6294e+01, -2.2936e+01,  1.6220e+01, -8.9817e+00,\n",
      "          -2.5780e+01, -7.4284e+00,  2.7535e+00, -2.6702e+01, -4.2498e+01,\n",
      "          -2.8285e+01, -1.5682e+01, -1.4853e+01, -8.3753e+00, -2.0579e+01,\n",
      "           3.1526e-01, -7.7811e-01,  2.5956e+01],\n",
      "         [ 0.0000e+00,  2.5873e+01,  3.3898e+01,  5.9913e+01,  2.9806e+01,\n",
      "           8.8440e+00,  3.7346e+01,  9.2127e+01,  4.0057e+00,  1.1286e+01,\n",
      "           1.9310e+01,  2.7802e+01,  4.4558e+01,  4.4471e+01,  1.4081e+01,\n",
      "           1.2729e+01,  2.6599e+01,  3.5491e+01],\n",
      "         [ 0.0000e+00,  3.4649e+01,  1.1739e+01,  3.1017e+01,  2.0094e+01,\n",
      "          -4.7955e+00,  1.9851e+01,  5.8858e+01,  3.5491e+00, -5.0184e+00,\n",
      "           9.8593e+00,  1.9867e+01,  3.5531e+01,  3.1202e+01, -1.4234e+01,\n",
      "          -6.0533e+00, -1.5831e+00,  1.7014e+01],\n",
      "         [ 0.0000e+00,  3.0113e+01, -2.9733e+01,  2.2145e+01,  6.1962e+00,\n",
      "          -1.3295e+01, -6.3196e+00, -2.7320e+00, -4.6817e+01, -2.8756e+01,\n",
      "          -5.0369e+00,  2.6770e+00,  8.1368e+00, -1.2267e+00, -4.1157e+01,\n",
      "          -3.5680e+01, -2.8246e+01, -4.3648e+00],\n",
      "         [ 0.0000e+00,  2.5371e+01,  2.3916e+00,  3.1304e+01,  2.4415e+01,\n",
      "           1.3317e+00,  1.9229e+01,  4.0573e+01, -5.2559e+01, -2.0863e+01,\n",
      "          -6.1420e+00,  2.2059e+01,  3.0564e+01,  5.7457e+00, -3.3555e+01,\n",
      "          -2.0529e+01, -8.7982e+00,  1.2072e+01]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([3, 17, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/15p7683j6vz_5pj6pk23sx1c0000gn/T/ipykernel_97008/1440010210.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens.input_ids = torch.tensor(tokens.input_ids)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model = DistilBertForParsing.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "sentences = TRAIN_DATA[531:534]\n",
    "\n",
    "encoded_inputs: BatchEncoding = []\n",
    "token_spans_list: list[list[TokenSpan]] = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "encoded_inputs, token_spans_list = encode(TOKENIZER, sentences)\n",
    "\n",
    "out = model.forward(encoded_inputs, token_spans_list)\n",
    "\n",
    "print(out)\n",
    "print(out.shape) # should be (1, 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now almost ready to train the parser. The missing piece is a data collator that prepares a batch of parsed sentences:\n",
    "\n",
    "* tokenises the sentences and extracts token spans using `encode()` (Problem&nbsp;1)\n",
    "* constructs the ground-truth head tensor needed to compute the loss (Problem&nbsp;2)\n",
    "\n",
    "The code in the next cell implements these two steps. For pseudo-words introduced through padding, we assign a head position of −100. This value is ignored by PyTorch’s cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ParserBatcher(object):\n",
    "\n",
    "    def __init__(self, tokenizer, device=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, parser_inputs):\n",
    "        encoded_input, start_indices = encode(self.tokenizer, parser_inputs)\n",
    "\n",
    "        # Get the maximal number of words, for padding\n",
    "        max_num_words = max(len(s) for s in parser_inputs)\n",
    "\n",
    "        # Construct tensor containing the ground-truth heads\n",
    "        all_heads = []\n",
    "        for parser_input in parser_inputs:\n",
    "            words, heads = zip(*parser_input)\n",
    "            heads = list(heads)\n",
    "            heads.extend([-100] * (max_num_words - len(heads)))  # -100 will be ignored\n",
    "            all_heads.append(heads)\n",
    "        all_heads = torch.LongTensor(all_heads)\n",
    "\n",
    "        # Send all data to the specified device\n",
    "        if self.device:\n",
    "            encoded_input = encoded_input.to(self.device)\n",
    "            all_heads = all_heads.to(self.device)\n",
    "\n",
    "        return encoded_input, start_indices, all_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the training loop of the parser. Most of it is quite standard. The training loss of the parser is the cross-entropy between the head scores and the ground truth head positions. In other words, the parser is trained as a classifier that predicts the position of each word&rsquo;s head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DEVICE = torch.device('mps')\n",
    "\n",
    "def train(dataset, n_epochs=1, lr=1e-5, batch_size=8):\n",
    "    # Initialise the tokeniser\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Initialise the encoder\n",
    "    model = DistilBertForParsing.from_pretrained('distilbert-base-uncased').to(DEVICE)\n",
    "\n",
    "    # Initialise the data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n",
    "\n",
    "    # Initialise the optimiser\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train for the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # We keep track of the running loss\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        with tqdm(total=len(dataset)) as pbar:\n",
    "            pbar.set_description(f'Epoch {epoch+1}')\n",
    "\n",
    "            # Process a batch of samples\n",
    "            for encoded_input, token_spans, gold_heads in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the arc scores\n",
    "                arc_scores = model.forward(encoded_input, token_spans)\n",
    "                # shape: [batch_size, num_words, num_words+1]\n",
    "\n",
    "                # Flatten arc_scores and gold_heads for cross_entropy\n",
    "                loss = F.cross_entropy(arc_scores.flatten(0, -2), gold_heads.view(-1))\n",
    "                # shape of the flattened arc_scores: [batch_size * num_words, num_words+1]\n",
    "                # shape of the flattened gold_heads: [batch_size * num_words]\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the loss\n",
    "                running_loss += loss.item()\n",
    "                n_batches += 1\n",
    "                pbar.set_postfix(loss=running_loss/n_batches)\n",
    "                pbar.update(len(token_spans))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the parser. With a GPU, you should expect training times of approximately 3&nbsp;minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForParsing were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['biaffine.bias', 'biaffine.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:   0%|          | 0/12544 [00:00<?, ?it/s]/var/folders/_5/15p7683j6vz_5pj6pk23sx1c0000gn/T/ipykernel_97008/1440010210.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens.input_ids = torch.tensor(tokens.input_ids)\n",
      "Epoch 1:   0%|          | 0/12544 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PARSING_MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[133], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, n_epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(arc_scores\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), gold_heads\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# shape of the flattened arc_scores: [batch_size * num_words, num_words+1]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# shape of the flattened gold_heads: [batch_size * num_words]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Update the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/TDDE09/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/TDDE09/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PARSING_MODEL = train(TRAIN_DATA, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Evaluation (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsers are commonly evaluated using **unlabelled attachment score (UAS)**, which is the percentage of (non-root) words that have been assigned their correct heads. The following cell contains skeleton code for a function `uas()` that computes this score on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(tokenizer, model, filename, batch_size=8):\n",
    "    # Compute score on a given dataset\n",
    "    dataset = ParserDataset(filename)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer))\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `uas()` function to match the following specification:\n",
    "\n",
    "**uas** (*tokenizer*, *model*, *filename*)\n",
    "\n",
    "> Takes a tokenizer (*tokenizer*), a trained parsing model (*model*), and the filename of a dataset in the CoNLLU format (*filename*) and returns the unlabelled attachment score of the model on the tokenised dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️ Note that pseudo-words corresponding to padding must be excluded from the calculation of the UAS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell evaluates the trained parser on the development section of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uas(TOKENIZER, PARSING_MODEL, ParserDataset('en_ewt-ud-dev.conllu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your notebook must contain output demonstrating at least 86% UAS on the development data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Counting trees (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last problem of this lab, we ask you to take a closer look at the graph-theoretic properties of the outputs of your trained model.\n",
    "\n",
    "The following cell contains code that will use the tokeniser and your trained model to parse the development data and write the gold-standard dependency trees as well as the dependency trees predicted by the parser to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = ParserDataset('en_ewt-ud-dev.conllu')\n",
    "data_loader = DataLoader(dataset, collate_fn=ParserBatcher(TOKENIZER, device=DEVICE))\n",
    "\n",
    "with open('graphs.jsonl', 'wt') as fp:\n",
    "    for encoded_input, token_spans, gold_heads in data_loader:\n",
    "        with torch.no_grad():\n",
    "            head_scores = PARSING_MODEL.forward(encoded_input, token_spans)\n",
    "            pred_heads = torch.argmax(head_scores, dim=-1)\n",
    "        for gold, pred in zip(gold_heads, pred_heads):\n",
    "            mask = gold.ne(-100)\n",
    "            print(json.dumps((gold[mask].tolist(), pred[mask].tolist())), file=fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab directory contains a script `analyze.py` that you can use to compute statistics on various graph-theoretic properties: total number of graphs, number of acyclic graphs, connected graphs, trees, and projective trees. Document your exploration in a short reflection piece (ca.&nbsp;150 words). Respond to the following prompts\n",
    "\n",
    "* What are the results of this exploration? Summarise the statistics you obtained.\n",
    "* Based on what you know about the parser, did you expect your results? Was there anything surprising in them?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🥳 Congratulations on finishing this lab! 🥳**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_Parser Panic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
