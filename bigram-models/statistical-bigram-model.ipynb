{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b684dd6",
   "metadata": {},
   "source": [
    "# Statistical bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8f0f1",
   "metadata": {},
   "source": [
    "In this notebook, we will build our first language model. Large language models such as GPT-4 generate text as a sequence of words. The language model presented here is very small and generates text as a sequence of individual characters. Also, it is not built on a neural network but is based on probabilities defined on pairs of adjacent characters. Because of this, it is known as a **statistical bigram model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b28ad",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32d3b6",
   "metadata": {},
   "source": [
    "The task we will address in this notebook is to generate person names. The data for this task comes as a text file containing Swedish first names (*tilltalsnamn*). More specifically, the file lists the most frequent names as of 2022-12-31 in decreasing order of frequency. The raw data was obtained from [Statistics Sweden](https://www.scb.se/) and postprocessed by lowercasing each name.\n",
    "\n",
    "We start by opening the file and store its contents in a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64664b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', encoding='utf-8') as fp:    \n",
    "    names = [line.rstrip() for line in fp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878a612",
   "metadata": {},
   "source": [
    "Here are the five most frequent names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb5b1491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:10]\n",
    "names.index('axel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889ff8e",
   "metadata": {},
   "source": [
    "In total, we have 32K names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255dd353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a9c3b",
   "metadata": {},
   "source": [
    "> **🤔 Problem 1: What’s in the data?**\n",
    ">\n",
    "> It is important to engage with the data you are working with. One way to do so is to ask questions. Is your own name included in the dataset? Is your name frequent or rare? Can you tell from a name whether the person with that name is male or female? Can you tell whether they are immigrants? What would be ethical and unethical uses of systems that *can* tell this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16af3f",
   "metadata": {},
   "source": [
    "## Character-to-index mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377cd22",
   "metadata": {},
   "source": [
    "We create a string-to-integer mapping from the characters (letters) in the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60e6cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$abcdefghijklmnopqrstuvwxyzàáâãäåæçèéêëíîïñòóôöøúüý\n"
     ]
    }
   ],
   "source": [
    "char2idx = {'$': 0}\n",
    "\n",
    "for name in names:\n",
    "    for char in name:\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)\n",
    "\n",
    "print(''.join(sorted(char2idx.keys()))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9b820",
   "metadata": {},
   "source": [
    "Note that we reserve a special character `$` with index&nbsp;0. We will use this character to mark the start and the end of a sequence of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4cf23",
   "metadata": {},
   "source": [
    "> **🤯 What does this code do?**\n",
    ">\n",
    "> Throughout the course, we often present code without detailed explanations – we assume you can follow along even so. If you need help understanding code, you can often get useful explanations from AI assistant services. For example, the following explanation of the above code was generated by ChatGPT&nbsp;3.5 (and lightly edited) as a reponse to the prompt *Explain the following code*.\n",
    ">\n",
    "> “The code begins by initializing a dictionary called `char2idx` with a special character `$` mapped to the index&nbsp;0. It then iterates through a collection of names, and further through each character in the name. Inside the nested loop, there is a conditional statement that checks if the character is already present in the dictionary. If it is not, the code assigns a unique index to that character. The index is determined by the current length of the dictionary, effectively assigning consecutive indices starting from&nbsp;1 (since `$` already occupies index&nbsp;0).”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfa279",
   "metadata": {},
   "source": [
    "> **🤔 Problem 2: A look inside the vocabulary**\n",
    ">\n",
    "> Write code that prints the vocabulary for the names dataset. Would you have expected this vocabulary? Would you expect the same vocabulary for a list of English names? What would you expect regarding the frequency distribution of the characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356723cc",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969b408",
   "metadata": {},
   "source": [
    "As already mentioned, our model is based on pairs of contiguous characters. Such pairs are called **bigrams**. For example, the bigrams in the name *anna* are *an*, *nn*, and *na*. In addition to the standard characters, we also include the start and end marker `$`.\n",
    "\n",
    "The code in the following cell generates all bigrams from a given iterable of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f255c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(names):\n",
    "    for name in names:\n",
    "        for x, y in zip('$' + name, name + '$'):\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca4a57",
   "metadata": {},
   "source": [
    "For example, here are the bigrams extracted from the first two names in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8433fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', 'n'),\n",
       " ('n', 'a'),\n",
       " ('a', '$'),\n",
       " ('$', 'm'),\n",
       " ('m', 'a'),\n",
       " ('a', 'r'),\n",
       " ('r', 'i'),\n",
       " ('i', 'a'),\n",
       " ('a', '$')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b for b in bigrams(names[:2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039626f7",
   "metadata": {},
   "source": [
    "> **🤯 Generator functions**\n",
    ">\n",
    "> Note that `bigrams()` is a **generator function**: It does not `return` a list of all bigrams, it `yield`s all bigrams. This is more efficient in terms of memory usage, especially when dealing with the larger datasets we will encounter in this course. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075264e3",
   "metadata": {},
   "source": [
    "## Estimating bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b767e85",
   "metadata": {},
   "source": [
    "How does a language model generate text? Intuitively, we can imagine rolling a multi-sided dice whose sides correspond to the elements of the vocabulary. Whereas standard dice are fair (all sides land face up with the same uniform probability), the dice of language models are weighted.\n",
    "\n",
    "**The basic idea behind a bigram model is to let the probability of the next element in a generated sequence depend on the previous element.** One can think of the model as consisting of several differently weighted dice, one for each preceding character.\n",
    "\n",
    "To estimate the probabilities of a bigram model, we start by counting how often each bigram occurs in the dataset. We can keep track of these counts by arranging them in a matrix&nbsp;$M$. More formally, let $V = \\{c_0, \\dots, c_{n-1}\\}$ be our character vocabulary, where $c_0 = \\$$. Then the matrix entry $M_{ij}$ should be the count of the character bigram $c_ic_j$ in our list of names. To compute this matrix, we can use the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf3e27cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 3647, 1721,  ...,    0,    0,    0],\n",
       "        [8599,  541, 5147,  ...,    0,    0,    0],\n",
       "        [4349, 3058,  810,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,    0,    1,  ...,    0,    0,    0],\n",
       "        [   2,    0,    0,  ...,    0,    0,    0],\n",
       "        [   0,    0,    0,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a counts matrix with all zeros\n",
    "counts = torch.zeros(len(char2idx), len(char2idx))\n",
    "\n",
    "# Update the counts based on the bigrams\n",
    "for x, y in bigrams(names):\n",
    "    counts[char2idx[x]][char2idx[y]] += 1\n",
    "\n",
    "counts.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82e3ed",
   "metadata": {},
   "source": [
    "Note that we represent matrices as *tensors* from the [PyTorch library](https://pytorch.org/). You will learn more about this library later in the course.\n",
    "\n",
    "Now that we have the bigram counts, we are ready to define our bigram model. This model is essentially a conditional probability distribution over all possible next characters, given the immediately preceding character. Formally, using the notation introduced above, the model consists of probabilities of the form $P(c_j|c_i)$. To compute them, we divide each bigram count $M_{ij}$ by the sum along the row $M_{i:}$. We can accomplish this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a75cb80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = counts / counts.sum(dim=-1, keepdim=True)\n",
    "\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "# highest probability following char å\n",
    "idx2char[model[char2idx['å']].argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe749cf",
   "metadata": {},
   "source": [
    "> **🤔 Problem 3: Inspecting the model**\n",
    ">\n",
    "> In a name, which letter is most likely to come after the letter `a`? Which letter is most likely to start or end a name? Can you give an example of a name that is impossible according to our bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d9ba1",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b651c2a",
   "metadata": {},
   "source": [
    "Now that we have estimated the probabilities of our bigram model, we can generate text. To do so, we repeatedly “roll a dice” by sampling from the next-character distribution, conditioning on the previous character. Equivalently, we can think of sampling from a collection of categorial distributions over the next character, one distribution per previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51db917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brdi\n",
      "a\n",
      "fa\n",
      "zene\n",
      "l\n"
     ]
    }
   ],
   "source": [
    "# Construct the inverse of the character-to-index mapping\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "# Generate 5 samples\n",
    "for _ in range(5):\n",
    "\n",
    "    # We begin with the start-of-sequence marker\n",
    "    generated = '$'\n",
    "\n",
    "    while True:\n",
    "        # Look up the integer index of the previous character\n",
    "        previous_idx = char2idx[generated[-1]]\n",
    "\n",
    "        # Get the relevant probability distribution\n",
    "        probs = model[previous_idx]\n",
    "\n",
    "        # Sample an index from the distribution\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Get the corresponding character\n",
    "        next_char = idx2char[next_idx]\n",
    "        \n",
    "\n",
    "        # Break if the model generates the end-of-sequence marker\n",
    "        if next_char == '$':\n",
    "            break\n",
    "\n",
    "        # Add the next character to the output\n",
    "        generated = generated + next_char\n",
    "\n",
    "    # Print the generated output (without the end-of-sequence marker)\n",
    "    print(generated[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacff22f",
   "metadata": {},
   "source": [
    "As we can see, the strings generated by our bigram model only vaguely resemble actual names. This should not really surprise us: after all, each next character is generated by only looking at the immediately preceding character, which is too short a context to model many important aspects of names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0423a52",
   "metadata": {},
   "source": [
    "> **🤔 Problem 4: Probability of a name**\n",
    ">\n",
    "> What is the probability for it to generate your name? What is the probability for our model to generate the single-letter “name” `s`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c916c9f",
   "metadata": {},
   "source": [
    "## Evaluating language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f4996",
   "metadata": {},
   "source": [
    "Language models are commonly evaluated by computing their **perplexity**, which can be thought of as a measure of how “surprised” the model is when being exposed to a text. The larger the perplexity on a given text, the less likely it is that the model would have generated that text.\n",
    "\n",
    "To compute the perplexity of our bigram model on a reference text, we first compute the average negative log likelihood that the model assigns to a gold-standard next character after having seen the previous character. To get the perplexity, we then exponentiate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eed97cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Collect the negative log likelihoods\n",
    "nlls = []\n",
    "for prev_char, next_char in bigrams(names):\n",
    "    prev_i = char2idx[prev_char]\n",
    "    prev_j = char2idx[next_char]\n",
    "    nlls.append(- math.log(model[prev_i][prev_j]))\n",
    "\n",
    "# Compute the average\n",
    "avg_nll = sum(nlls) / len(nlls)\n",
    "\n",
    "# Print as perplexity\n",
    "print(f'{math.exp(avg_nll):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb3029",
   "metadata": {},
   "source": [
    "> **🤔 Problem 5: Upper bound on perplexity**\n",
    ">\n",
    "> The perplexity of our bigram model can be read as the average number of characters the model must choose from when trying to “guess” the held-out text. The lowest possible perplexity value is&nbsp;1, which corresponds to the case where each next character is completely certain and no guessing is necessary. What is a reasonable *upper bound* on the perplexity of our bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb2241",
   "metadata": {},
   "source": [
    "That’s all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
